{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bfc818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01d3246",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4a9488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The pre-processing pipeline inherits from the original ClinicalBERT with minor changes. see https://github.com/kexinhuang12345/clinicalBERT\n",
    "path = 'mimic_data_directory'\n",
    "df_notes = pd.read_csv('/NOTEEVENTS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c04a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# held-out dataset if you want to fine-tune on MIMIC data, it is better to exclude them prior to the training\n",
    "# exclude all data in test set for re-admission task where we are interested in \n",
    "df_test_ids = pd.read_csv('discharge/test.csv').ID.unique()\n",
    "df_notes= df_notes[~df_notes.HADM_ID.isin(df_test_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1499bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose interested categories, for more information, please refer to \n",
    "category_list = ['Discharge summary', 'Echo', 'Nursing', 'Physician ',\n",
    "       'Rehab Services', 'Respiratory ', 'Nutrition',\n",
    "       'General', 'Pharmacy', 'Consult', 'Radiology',\n",
    "       'Nursing/other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8063f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notes = df_notes[df_notes.CATEGORY.isin(category_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7e012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess1(x):\n",
    "    y=re.sub('\\\\[(.*?)\\\\]','',x) \n",
    "    y=re.sub('[0-9]+\\. ','',y) \n",
    "    y=re.sub('dr\\.','doctor',y)\n",
    "    y=re.sub('m\\.d\\.','md',y)\n",
    "    y=re.sub('admission date:','',y)\n",
    "    y=re.sub('discharge date:','',y)\n",
    "    y=re.sub('--|__|==','',y)\n",
    "    #more substituion can be made to align with general knowledge such as \"p.o.\" to \"by mouth\"\n",
    "    \n",
    "    # remove, spaces\n",
    "    y = y.translate(str.maketrans(\"\", \"\"))\n",
    "    y = \" \".join(y.split())\n",
    "    return y\n",
    "\n",
    "def preprocessing(df_notes): \n",
    "    df_notes['TEXT']=df_notes['TEXT'].fillna(' ')\n",
    "    df_notes['TEXT']=df_notes['TEXT'].str.replace('\\n',' ')\n",
    "    df_notes['TEXT']=df_notes['TEXT'].str.replace('\\r',' ')\n",
    "    df_notes['TEXT']=df_notes['TEXT'].apply(str.strip)\n",
    "    #We use uncased text which is also used in PubMedBERT\n",
    "    df_notes['TEXT']=df_notes['TEXT'].str.lower()\n",
    "\n",
    "    df_notes['TEXT']=df_notes['TEXT'].apply(lambda x: preprocess1(x))\n",
    "    \n",
    "    return df_notes\n",
    "\n",
    "df_notes_processed= preprocessing(df_notes)\n",
    "# to reuse the processed data in other tasks and save time\n",
    "df_notes_processed.to_csv('df_notes_processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7e660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a93071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toSentence(x):\n",
    "    doc = nlp(x)\n",
    "    text=[]\n",
    "    try:\n",
    "        for sent in doc.sents:\n",
    "            st=str(sent).strip() \n",
    "            if len(st)<30:\n",
    "                #Merging too-short sentences to appropriate length, this is inherited from ClinicalBERT with changes in merged length \n",
    "                if len(text)!=0:\n",
    "                    text[-1]=' '.join((text[-1],st))\n",
    "                else:\n",
    "                    text=[st]\n",
    "            else:\n",
    "                text.append((st))\n",
    "    except:\n",
    "        print(doc)\n",
    "    return text\n",
    "\n",
    "pretrain_sent=df_notes_processed['TEXT'].apply(lambda x: toSentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c888c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'put your data path here'\n",
    "file=open(data_path + '/clinical_sentences_pretrain_wo_ECG_30_length_down_sampled.txt','w')\n",
    "pretrain_sent = pretrain_sent.values\n",
    "#random sample 500,000 documents \n",
    "pretrain_sent = np.random.choice(pretrain_sent,500000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49faabb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(data_path + '/clinical_sentences_pretrain_wo_ECG_30_length_truncated_500000.txt','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b68c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the txt file for building dataset, empty lines between docs (for NSP task)\n",
    "for i in tqdm(range(len(pretrain_sent))):\n",
    "    if len(pretrain_sent[i]) > 0:\n",
    "        # remove the one token note\n",
    "        note = pretrain_sent[i]\n",
    "        for sent in note:\n",
    "            file.write(sent+'\\n')\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cc9e46",
   "metadata": {},
   "source": [
    "# Train Tokenizer\n",
    "Only when you pretrain from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7c57cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d3e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [str(x) for x in Path(data_path).glob(\"*.txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2878b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcf6c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "tokenizer.save_model(\".\", \"Tokenizer_Name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff48cee",
   "metadata": {},
   "source": [
    "# Clinical-PubMedBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34ef7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from transformers import TextDataset\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorForWholeWordMask\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219c8362",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b723c8dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='your text data path',\n",
    "    block_size=128,\n",
    "    # You can also use 512 block_size to train the model, also adjust batch size.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429b338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Whole Word Masking instead of ordinary masking\n",
    "data_collator = DataCollatorForWholeWordMask(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c140303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use 5000 steps to warm-up, other optimization parameters are default\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"your_output_directory\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=2_500,\n",
    "    save_total_limit=3,\n",
    "    prediction_loss_only=True,\n",
    "    warmup_steps = 5000\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50f44f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471c0100",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"your_model_directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b38b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can try some examples to check the learned model!\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"your_model_directory\",\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
