{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ee96e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e516816f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 09:07:48.000 | WARNING  | __main__:<module>:125 - Unfreezing the plm - will be updated during training\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20786/796958527.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    139\u001b[0m def train_mimic(prompt_model, train_dataloader, num_epochs, mode = \"train\", \n\u001b[1;32m    140\u001b[0m                 \u001b[0mckpt_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"icd9_triage\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                 \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 config = None):\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_dir' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,6,7'\n",
    "\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from typing import Dict\n",
    "from tqdm import tqdm\n",
    "from openprompt.data_utils import PROCESSORS\n",
    "import torch\n",
    "from openprompt.data_utils.utils import InputExample\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "from openprompt import PromptDataLoader\n",
    "from openprompt.prompts import ManualVerbalizer, ManualTemplate, SoftVerbalizer\n",
    "\n",
    "from openprompt.prompts import SoftTemplate, MixedTemplate\n",
    "from openprompt import PromptForClassification\n",
    "\n",
    "\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup,get_constant_schedule_with_warmup  # use AdamW is a standard practice for transformer \n",
    "from transformers.optimization import Adafactor, AdafactorSchedule  # use Adafactor is the default setting for T5\n",
    "\n",
    "# from openprompt.utils.logging import logger\n",
    "from loguru import logger\n",
    "\n",
    "from utils import Mimic_ICD9_Processor, Mimic_ICD9_Triage_Processor, Mimic_Mortality_Processor, customPromptDataLoader\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from utils import SummaryWriter # this version is better for logging hparams with metrics..\n",
    "\n",
    "import torchmetrics.functional.classification as metrics\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix, roc_auc_score \n",
    "\n",
    "from torch.utils.data.sampler import RandomSampler, WeightedRandomSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "import json\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import os \n",
    "# # Kill all processes on GPU 6 and 7\n",
    "# os.system(\"\"\"kill $(nvidia-smi | awk '$5==\"PID\" {p=1} p && $2 >= 6 && $2 < 7 {print $5}')\"\"\")\n",
    "\n",
    "'''\n",
    "Script to run different setups of prompt learning.\n",
    "\n",
    "Right now this is primarily set up for the mimic_top50_icd9 task, although it is quite flexible to other datasets. Any datasets need a corresponding processor class in utils.\n",
    "\n",
    "\n",
    "example usage. python prompt_experiment_runner.py --model bert --model_name_or_path bert-base-uncased --num_epochs 10 --tune_plm\n",
    "\n",
    "other example usage:\n",
    "- python prompt_experiment_runner.py --model t5 --model_name_or_path razent/SciFive-base-Pubmed_PMC --num_epochs 10 --template_id 0 --template_type soft --max_steps 15000 --tune_plm\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "from openprompt.utils.reproduciblity import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "from openprompt.plms.seq2seq import T5TokenizerWrapper, T5LMTokenizerWrapper\n",
    "from transformers import T5Config, T5Tokenizer, T5ForConditionalGeneration\n",
    "from openprompt.data_utils.data_sampler import FewShotSampler\n",
    "from openprompt.plms import load_plm\n",
    "\n",
    "\n",
    "# set up some variables to add to checkpoint and logs filenames\n",
    "time_now = str(datetime.now().strftime(\"%d-%m-%Y--%H-%M\"))\n",
    "version = f\"version_{time_now}\"\n",
    "\n",
    "\n",
    "dataset = \"icd9_triage\"\n",
    "model_name_or_path = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "template_id = 2\n",
    "verbalizer_id = 0\n",
    "template_type = 'mixed'\n",
    "verbalizer_type = 'soft'\n",
    "model = 'bert'\n",
    "scripts_path = './scripts/'\n",
    "plm_lr = 1e-05\n",
    "prompt_lr = 0.3\n",
    "warmup_step_prompt = 50\n",
    "plm_warmup_steps = 50\n",
    "num_epochs = 5\n",
    "num_samples = 5\n",
    "batch_size = 4\n",
    "gpu_num = 0\n",
    "optimizer=\"adafactor\" \n",
    "training_size = \"fewshot\"\n",
    "tune_plm = True\n",
    "project_root = '/mnt/sdg/niallt/saved_models/mimic-tasks/prompt-based-models/'\n",
    "few_shot_n = 32\n",
    "\n",
    "    # actually want to save the checkpoints and logs in same place now. Becomes a lot easier to manage later\n",
    "if tune_plm:\n",
    "    logger.warning(\"Unfreezing the plm - will be updated during training\")\n",
    "    freeze_plm = False\n",
    "    # set checkpoint, logs and params save_dirs    \n",
    "    logs_dir = f\"{project_root}/raytune_results/{dataset}/{model_name_or_path}_temp{template_type}{template_id}_verb{verbalizer_type}{verbalizer_id}_{training_size}_{few_shot_n}/{version}\"\n",
    "\n",
    "else:\n",
    "    logger.warning(\"Freezing the plm\")\n",
    "    freeze_plm = True\n",
    "    # set checkpoint, logs and params save_dirs    \n",
    "    logs_dir = f\"{project_root}/raytune_results/{dataset}/frozen_plm/{model_name_or_path}_temp{template_type}{template_id}_verb{verbalizer_type}{verbalizer_id}_{training_size}_{few_shot_n}/{version}\"\n",
    "\n",
    "# set up tensorboard logger\n",
    "writer = SummaryWriter(logs_dir)\n",
    "\n",
    "def train_mimic(prompt_model, train_dataloader, num_epochs, mode = \"train\", \n",
    "                ckpt_dir = None, dataset = \"icd9_triage\",\n",
    "                data_dir = data_dir,\n",
    "                config = None):\n",
    "\n",
    "    \n",
    "\n",
    "    plm, tokenizer, model_config, WrapperClass = load_plm(model, model_name_or_path)\n",
    "\n",
    "    # edit based on whether or not plm was frozen during training\n",
    "\n",
    "\n",
    "    # initialise empty dataset\n",
    "    dataset = {}\n",
    "\n",
    "    # crude setting of sampler to None - changed for mortality with umbalanced dataset\n",
    "\n",
    "    sampler = None\n",
    "    # Below are multiple dataset examples, although right now just mimic ic9-top50. \n",
    "    if dataset == \"icd9_triage\":\n",
    "        logger.warning(f\"Using the following dataset: {dataset} \")\n",
    "        Processor = Mimic_ICD9_Triage_Processor\n",
    "        # update data_dir\n",
    "        data_dir = f\"{data_dir}/triage\"\n",
    "\n",
    "        ce_class_weights = ce_class_weights\n",
    "        sampler_weights = sampler_weights    \n",
    "        balance_data = balance_data\n",
    "\n",
    "        # get different splits\n",
    "        dataset['train'] = Processor().get_examples(data_dir = data_dir, mode = \"train\")\n",
    "        dataset['validation'] = Processor().get_examples(data_dir = data_dir, mode = \"valid\")\n",
    "        dataset['test'] = Processor().get_examples(data_dir = data_dir, mode = \"test\")\n",
    "        # the below class labels should align with the label encoder fitted to training data\n",
    "        # you will need to generate this class label text file first using the mimic processor with generate_class_labels flag to set true\n",
    "        # e.g. Processor().get_examples(data_dir = data_dir, mode = \"train\", generate_class_labels = True)[:10000]\n",
    "        class_labels =Processor().load_class_labels()\n",
    "        print(f\"number of classes: {len(class_labels)}\")\n",
    "        scriptsbase = f\"{scripts_path}/mimic_triage/\"\n",
    "        scriptformat = \"txt\"\n",
    "        max_seq_l = 480 # this should be specified according to the running GPU's capacity \n",
    "        \n",
    "        batchsize_t = config['batch_size'] \n",
    "        batchsize_e = config['batch_size'] \n",
    "        gradient_accumulation_steps = config['gradient_accum_steps']\n",
    "        model_parallelize = False # if multiple gpus are available, one can use model_parallelize\n",
    "\n",
    "    else:\n",
    "        \n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    # Now define the template and verbalizer. \n",
    "    # Note that soft template can be combined with hard template, by loading the hard template from file. \n",
    "    # For example, the template in soft_template.txt is {}\n",
    "    # The choice_id 1 is the hard template \n",
    "\n",
    "    # decide which template and verbalizer to use\n",
    "    if template_type == \"manual\":\n",
    "        print(f\"manual template selected, with id :{template_id}\")\n",
    "        mytemplate = ManualTemplate(tokenizer=tokenizer).from_file(f\"{scriptsbase}/manual_template.txt\", choice=template_id)\n",
    "\n",
    "    elif template_type == \"soft\":\n",
    "        print(f\"soft template selected, with id :{template_id}\")\n",
    "        mytemplate = SoftTemplate(model=plm, tokenizer=tokenizer, num_tokens=soft_token_num, initialize_from_vocab=init_from_vocab).from_file(f\"{scriptsbase}/soft_template.txt\", choice=template_id)\n",
    "\n",
    "\n",
    "    elif template_type == \"mixed\":\n",
    "        print(f\"mixed template selected, with id :{template_id}\")\n",
    "        mytemplate = MixedTemplate(model=plm, tokenizer=tokenizer).from_file(f\"{scriptsbase}/mixed_template.txt\", choice=template_id)\n",
    "    # now set verbalizer\n",
    "    if verbalizer_type == \"manual\":\n",
    "        print(f\"manual verbalizer selected, with id :{verbalizer_id}\")\n",
    "        myverbalizer = ManualVerbalizer(tokenizer, classes=class_labels).from_file(f\"{scriptsbase}/manual_verbalizer.{scriptformat}\", choice=verbalizer_id)\n",
    "\n",
    "    elif verbalizer_type == \"soft\":\n",
    "        print(f\"soft verbalizer selected!\")\n",
    "        myverbalizer = SoftVerbalizer(tokenizer, plm, num_classes=len(class_labels))\n",
    "    # are we using cuda and if so which number of device\n",
    "    use_cuda = True\n",
    "    \n",
    "    cuda_device = \"cpu\"\n",
    "    if use_cuda:\n",
    "        if torch.cuda.is_available():\n",
    "            cuda_device = \"cuda:0\"\n",
    "    \n",
    "    \n",
    "            cuda_device = torch.device(f'cuda:{gpu_num}')\n",
    "    # now set the default gpu to this one\n",
    "    torch.cuda.set_device(cuda_device)\n",
    "\n",
    "\n",
    "    print(f\"tune_plm value: {tune_plm}\")\n",
    "    prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=freeze_plm, plm_eval_mode=plm_eval_mode)\n",
    "    if use_cuda:\n",
    "        prompt_model=  prompt_model.to(cuda_device)\n",
    "\n",
    "    if model_parallelize:\n",
    "        prompt_model.parallelize()\n",
    "\n",
    "\n",
    "    # if doing few shot learning - produce the datasets here:\n",
    "    if training_size == \"fewshot\":\n",
    "        logger.warning(f\"Will be performing few shot learning.\")\n",
    "    # create the few_shot sampler for when we want to run training and testing with few shot learning\n",
    "        support_sampler = FewShotSampler(num_examples_per_label = few_shot_n, also_sample_dev=False)\n",
    "\n",
    "        # create a fewshot dataset from training, val and test. Seems to be what several papers do...\n",
    "        dataset['train'] = support_sampler(dataset['train'], seed=seed)\n",
    "        dataset['validation'] = support_sampler(dataset['validation'], seed=seed)\n",
    "        dataset['test'] = support_sampler(dataset['test'], seed=seed)\n",
    "\n",
    "    # are we doing training?\n",
    "    do_training = (not no_training)\n",
    "    if do_training:\n",
    "        # if we have a sampler .e.g weightedrandomsampler. Do not shuffle\n",
    "        if \"WeightedRandom\" in type(sampler).__name__:\n",
    "            logger.warning(\"Sampler is WeightedRandom - will not be shuffling training data!\")\n",
    "            shuffle = False\n",
    "        else:\n",
    "            shuffle = True\n",
    "        logger.warning(f\"Do training is True - creating train and validation dataloders!\")\n",
    "        train_dataloader = customPromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer, \n",
    "            tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3, \n",
    "            batch_size=batchsize_t,shuffle=shuffle, sampler = sampler, teacher_forcing=False, predict_eos_token=False,\n",
    "            truncate_method=\"tail\")\n",
    "\n",
    "        validation_dataloader = customPromptDataLoader(dataset=dataset[\"validation\"], template=mytemplate, tokenizer=tokenizer, \n",
    "            tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3, \n",
    "            batch_size=batchsize_e,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "            truncate_method=\"tail\")\n",
    "\n",
    "\n",
    "    # zero-shot test\n",
    "    test_dataloader = customPromptDataLoader(dataset=dataset[\"test\"], template=mytemplate, tokenizer=tokenizer, \n",
    "        tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3, \n",
    "        batch_size=batchsize_e,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "        truncate_method=\"tail\")\n",
    "\n",
    "\n",
    "    #TODO update this to handle class weights for imabalanced datasets\n",
    "    if ce_class_weights:\n",
    "        logger.warning(\"we have some task specific class weights - passing to CE loss\")\n",
    "        # get from the class_weight function\n",
    "        # task_class_weights = torch.tensor(task_class_weights, dtype=torch.float).to(cuda_device)\n",
    "        \n",
    "        # set manually cause above didnt work\n",
    "        task_class_weights = torch.tensor([1,16.1], dtype=torch.float).to(cuda_device)\n",
    "        loss_func = torch.nn.CrossEntropyLoss(weight = task_class_weights, reduction = 'mean')\n",
    "    else:\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # get total steps as a function of the max epochs, batch_size and len of dataloader\n",
    "    tot_step = max_steps\n",
    "\n",
    "    if tune_plm:\n",
    "        \n",
    "        logger.warning(\"We will be tuning the PLM!\") # normally we freeze the model when using soft_template. However, we keep the option to tune plm\n",
    "        no_decay = ['bias', 'LayerNorm.weight'] # it's always good practice to set no decay to biase and LayerNorm parameters\n",
    "        optimizer_grouped_parameters_plm = [\n",
    "            {'params': [p for n, p in prompt_model.plm.named_parameters() if (not any(nd in n for nd in no_decay))], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer_plm = AdamW(optimizer_grouped_parameters_plm, lr=config['plm_lr'])\n",
    "        scheduler_plm = get_linear_schedule_with_warmup(\n",
    "            optimizer_plm, \n",
    "            num_warmup_steps=plm_warmup_steps, num_training_steps=tot_step)\n",
    "    else:\n",
    "        logger.warning(\"We will not be tunning the plm - i.e. the PLM layers are frozen during training\")\n",
    "        optimizer_plm = None\n",
    "        scheduler_plm = None\n",
    "\n",
    "    # if using soft template\n",
    "    if template_type == \"soft\" or template_type == \"mixed\":\n",
    "        logger.warning(f\"{template_type} template used - will be fine tuning the prompt embeddings!\")\n",
    "        optimizer_grouped_parameters_template = [{'params': [p for name, p in prompt_model.template.named_parameters() if 'raw_embedding' not in name]}] # note that you have to remove the raw_embedding manually from the optimization\n",
    "        if optimizer.lower() == \"adafactor\":\n",
    "            optimizer_template = Adafactor(optimizer_grouped_parameters_template,  \n",
    "                                    lr=config['prompt_lr'],\n",
    "                                    relative_step=False,\n",
    "                                    scale_parameter=False,\n",
    "                                    warmup_init=False)  # when lr is 0.3, it is the same as the configuration of https://arxiv.org/abs/2104.08691\n",
    "            scheduler_template = get_constant_schedule_with_warmup(optimizer_template, num_warmup_steps=warmup_step_prompt) # when num_warmup_steps is 0, it is the same as the configuration of https://arxiv.org/abs/2104.08691\n",
    "        elif optimizer.lower() == \"adamw\":\n",
    "            optimizer_template = AdamW(optimizer_grouped_parameters_template, lr=config['prompt_lr']) # usually lr = 0.5\n",
    "            scheduler_template = get_linear_schedule_with_warmup(\n",
    "                            optimizer_template, \n",
    "                            num_warmup_steps=warmup_step_prompt, num_training_steps=tot_step) # usually num_warmup_steps is 500\n",
    "\n",
    "    elif template_type == \"manual\":\n",
    "        optimizer_template = None\n",
    "        scheduler_template = None\n",
    "\n",
    "\n",
    "    if verbalizer_type == \"soft\":\n",
    "        logger.warning(\"Soft verbalizer used - will be fine tuning the verbalizer/answer embeddings!\")\n",
    "        optimizer_grouped_parameters_verb = [\n",
    "        {'params': prompt_model.verbalizer.group_parameters_1, \"lr\":config['plm_lr']},\n",
    "        {'params': prompt_model.verbalizer.group_parameters_2, \"lr\":config['plm_lr']}        \n",
    "        ]\n",
    "        optimizer_verb= AdamW(optimizer_grouped_parameters_verb)\n",
    "        scheduler_verb = get_linear_schedule_with_warmup(\n",
    "                            optimizer_verb, \n",
    "                            num_warmup_steps=warmup_step_prompt, num_training_steps=tot_step) # usually num_warmup_steps is 500\n",
    "\n",
    "    elif verbalizer_type == \"manual\":\n",
    "        optimizer_verb = None\n",
    "        scheduler_verb = None\n",
    "\n",
    "\n",
    "    # set model to train \n",
    "    prompt_model.train()\n",
    "\n",
    "    # set up some counters\n",
    "    actual_step = 0\n",
    "    glb_step = 0\n",
    "\n",
    "    # some validation metrics to monitor\n",
    "    best_val_acc = 0\n",
    "    best_val_f1 = 0\n",
    "    best_val_prec = 0    \n",
    "    best_val_recall = 0\n",
    "\n",
    " \n",
    "\n",
    "    # this will be set to true when max steps are reached\n",
    "    leave_training = False\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print(f\"On epoch: {epoch}\")\n",
    "        tot_loss = 0 \n",
    "        epoch_loss = 0\n",
    "        for step, inputs in enumerate(train_dataloader):       \n",
    "\n",
    "            if use_cuda:\n",
    "                inputs = inputs.to(cuda_device)\n",
    "            logits = prompt_model(inputs)\n",
    "            labels = inputs['label']\n",
    "            loss = loss_func(logits, labels)\n",
    "\n",
    "            # normalize loss to account for gradient accumulation\n",
    "            loss = loss / gradient_accumulation_steps \n",
    "\n",
    "            # propogate backward to calculate gradients\n",
    "            loss.backward()\n",
    "            tot_loss += loss.item()\n",
    "\n",
    "            actual_step+=1\n",
    "            # log loss to tensorboard  every 50 steps    \n",
    "\n",
    "            # clip gradients based on gradient accumulation steps\n",
    "            if actual_step % gradient_accumulation_steps == 0:\n",
    "                # log loss\n",
    "                aveloss = tot_loss/(step+1)\n",
    "                # write to tensorboard\n",
    "                writer.add_scalar(\"train/batch_loss\", aveloss, glb_step)        \n",
    "\n",
    "                # clip grads            \n",
    "                torch.nn.utils.clip_grad_norm_(prompt_model.parameters(), 1.0)\n",
    "                glb_step += 1\n",
    "\n",
    "                # backprop the loss and update optimizers and then schedulers too\n",
    "                # plm\n",
    "                if optimizer_plm is not None:\n",
    "                    optimizer_plm.step()\n",
    "                    optimizer_plm.zero_grad()\n",
    "                if scheduler_plm is not None:\n",
    "                    scheduler_plm.step()\n",
    "                # template\n",
    "                if optimizer_template is not None:\n",
    "                    optimizer_template.step()\n",
    "                    optimizer_template.zero_grad()\n",
    "                if scheduler_template is not None:\n",
    "                    scheduler_template.step()\n",
    "                # verbalizer\n",
    "                if optimizer_verb is not None:\n",
    "                    optimizer_verb.step()\n",
    "                    optimizer_verb.zero_grad()\n",
    "                if scheduler_verb is not None:\n",
    "                    scheduler_verb.step()\n",
    "\n",
    "                # check if we are over max steps\n",
    "                if glb_step > max_steps:\n",
    "                    logger.warning(\"max steps reached - stopping training!\")\n",
    "                    leave_training = True\n",
    "                    break\n",
    "\n",
    "        # get epoch loss and write to tensorboard\n",
    "\n",
    "        epoch_loss = tot_loss/len(train_dataloader)\n",
    "        print(\"Epoch {}, loss: {}\".format(epoch, epoch_loss), flush=True)   \n",
    "        writer.add_scalar(\"train/epoch_loss\", epoch_loss, epoch)\n",
    "\n",
    "        \n",
    "        # run a run through validation set to get some metrics        \n",
    "        val_loss, val_acc, val_prec_weighted, val_prec_macro, val_recall_weighted,val_recall_macro, val_f1_weighted,val_f1_macro, val_auc_weighted,val_auc_macro, cm_figure = evaluate(prompt_model, validation_dataloader,\n",
    "                                                                                                                                                                                        use_cuda=use_cuda, cuda_device = cuda_device,\n",
    "                                                                                                                                                                                        loss_func = loss_func)\n",
    "\n",
    "        writer.add_scalar(\"valid/loss\", val_loss, epoch)\n",
    "        writer.add_scalar(\"valid/balanced_accuracy\", val_acc, epoch)\n",
    "        writer.add_scalar(\"valid/precision_weighted\", val_prec_weighted, epoch)\n",
    "        writer.add_scalar(\"valid/precision_macro\", val_prec_macro, epoch)\n",
    "        writer.add_scalar(\"valid/recall_weighted\", val_recall_weighted, epoch)\n",
    "        writer.add_scalar(\"valid/recall_macro\", val_recall_macro, epoch)\n",
    "        writer.add_scalar(\"valid/f1_weighted\", val_f1_weighted, epoch)\n",
    "        writer.add_scalar(\"valid/f1_macro\", val_f1_macro, epoch)\n",
    "\n",
    "        #TODO add binary classification metrics e.g. roc/auc\n",
    "        writer.add_scalar(\"valid/auc_weighted\", val_auc_weighted, epoch)\n",
    "        writer.add_scalar(\"valid/auc_macro\", val_auc_macro, epoch)        \n",
    "\n",
    "        # add cm to tensorboard\n",
    "        writer.add_figure(\"valid/Confusion_Matrix\", cm_figure, epoch)\n",
    "\n",
    "        # save checkpoint if validation accuracy improved\n",
    "        if val_acc >= best_val_acc:\n",
    "            # only save ckpts if no_ckpt is False - we do not always want to save - especially when developing code\n",
    "            if ckpt_dir != None:\n",
    "                logger.warning(\"Accuracy improved! Saving checkpoint!\")\n",
    "                torch.save(prompt_model.state_dict(),f\"{ckpt_dir}/best-checkpoint.ckpt\")\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "\n",
    "        if glb_step > max_steps:\n",
    "            leave_training = True\n",
    "            break\n",
    "    \n",
    "        if leave_training:\n",
    "            logger.warning(\"Leaving training as max steps have been met!\")\n",
    "            break \n",
    "\n",
    "        \n",
    "        # now we want to send these back to raytune\n",
    "        tune.report(loss=val_loss, accuracy = val_acc)\n",
    "           \n",
    "   \n",
    "# ## evaluate\n",
    "\n",
    "# %%\n",
    "\n",
    "def evaluate(prompt_model, dataloader, mode = \"validation\", \n",
    "                class_labels = None, use_cuda = True, cuda_device = None, loss_func= None):\n",
    "\n",
    "    prompt_model.eval()\n",
    "\n",
    "    tot_loss = 0\n",
    "    allpreds = []\n",
    "    alllabels = []\n",
    "    #record logits from the the model\n",
    "    alllogits = []\n",
    "    # store probabilties i.e. softmax applied to logits\n",
    "    allscores = []\n",
    "\n",
    "    allids = []\n",
    "    with torch.no_grad():\n",
    "        for step, inputs in enumerate(dataloader):\n",
    "            if use_cuda:\n",
    "                inputs = inputs.to(cuda_device)\n",
    "            logits = prompt_model(inputs)\n",
    "            labels = inputs['label']\n",
    "\n",
    "            loss = loss_func(logits, labels)\n",
    "            tot_loss += loss.item()\n",
    "\n",
    "            # add labels to list\n",
    "            alllabels.extend(labels.cpu().tolist())\n",
    "\n",
    "            # add ids to list - they are already a list so no need to send to cpu\n",
    "            allids.extend(inputs['guid'])\n",
    "\n",
    "            # add logits to list\n",
    "            alllogits.extend(logits.cpu().tolist())\n",
    "            #use softmax to normalize, as the sum of probs should be 1\n",
    "            # if binary classification we just want the positive class probabilities\n",
    "            if len(class_labels) > 2:  \n",
    "                allscores.extend(torch.nn.functional.softmax(logits).cpu().tolist())\n",
    "            else:\n",
    "\n",
    "                allscores.extend(torch.nn.functional.softmax(logits)[:,1].cpu().tolist())\n",
    "\n",
    "            # add predicted labels    \n",
    "            allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "    \n",
    "    val_loss = tot_loss/len(dataloader)    \n",
    "    # get sklearn based metrics\n",
    "    acc = balanced_accuracy_score(alllabels, allpreds)\n",
    "    f1_weighted = f1_score(alllabels, allpreds, average = 'weighted')\n",
    "    f1_macro = f1_score(alllabels, allpreds, average = 'macro')\n",
    "    prec_weighted = precision_score(alllabels, allpreds, average = 'weighted')\n",
    "    prec_macro = precision_score(alllabels, allpreds, average = 'macro')\n",
    "    recall_weighted = recall_score(alllabels, allpreds, average = 'weighted')\n",
    "    recall_macro = recall_score(alllabels, allpreds, average = 'macro')\n",
    "\n",
    "\n",
    "    # roc_auc  - only really good for binary classification but can try for multiclass too\n",
    "    # use scores instead of predicted labels to give probs\n",
    "    \n",
    "    if len(class_labels) > 2:   \n",
    "        roc_auc_weighted = roc_auc_score(alllabels, allscores, average = \"weighted\", multi_class = \"ovr\")\n",
    "        roc_auc_macro = roc_auc_score(alllabels, allscores, average = \"macro\", multi_class = \"ovr\")\n",
    "                  \n",
    "    else:\n",
    "        roc_auc_weighted = roc_auc_score(alllabels, allscores, average = \"weighted\")\n",
    "        roc_auc_macro = roc_auc_score(alllabels, allscores, average = \"macro\")         \n",
    "\n",
    "\n",
    "    \n",
    "   \n",
    "    return val_loss, acc, prec_weighted, prec_macro, recall_weighted, recall_macro, f1_weighted, f1_macro, roc_auc_weighted, roc_auc_macro\n",
    "\n",
    "\n",
    "\n",
    "# create raytune config\n",
    "config = {\n",
    "    \"plm_lr\":tune.loguniform(1e-4, 1e-5),\n",
    "    \"prompt_lr\":tune.loguniform(1e-4, 1e-5),\n",
    "    \"batch_size\": tune.choice([4]),\n",
    "    \"grad_accum_steps\":tune.choice([2,5,10]),\n",
    "    \"dropout\": tune.choice([0.1,0.2,0.5]),\n",
    "    \"optimizer\": tune.choice(['adamw']),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# create the trainable ray tune class\n",
    "trainable = tune.with_parameters(\n",
    "    train_mimic,   \n",
    "    num_epochs=num_epochs,\n",
    "    num_gpus=gpus_per_trial,\n",
    "    data_dir = data_dir)\n",
    "# run the analysis\n",
    "analysis = tune.run(\n",
    "    trainable,\n",
    "    resources_per_trial={\n",
    "        \"cpu\": 1,\n",
    "        \"gpu\": gpus_per_trial\n",
    "    },\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    config=config,    \n",
    "    num_samples=num_samples,\n",
    "    local_dir = f\"{save_dir}\",\n",
    "    name=f\"tune_mimic_{dataset}\"\n",
    "    \n",
    "    )\n",
    "import ray\n",
    "ray.shutdown()\n",
    "\n",
    "\n",
    "print(f\"Best config based on ray tune analysis!:\\n {analysis.best_config}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3-7-NLP",
   "language": "python",
   "name": "3-7-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
