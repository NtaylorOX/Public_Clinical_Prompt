{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "371d14d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from openprompt.data_utils import PROCESSORS\n",
    "import torch\n",
    "from openprompt.data_utils.utils import InputExample\n",
    "import argparse\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "from openprompt import PromptDataLoader\n",
    "from openprompt.prompts import ManualVerbalizer, ManualTemplate, MixedTemplate, SoftVerbalizer\n",
    "\n",
    "from openprompt.prompts import SoftTemplate\n",
    "from openprompt import PromptForClassification\n",
    "\n",
    "from openprompt.plms.seq2seq import T5TokenizerWrapper, T5LMTokenizerWrapper\n",
    "from transformers import T5Config, T5Tokenizer, T5ForConditionalGeneration\n",
    "from openprompt.data_utils.data_sampler import FewShotSampler\n",
    "from openprompt.plms import load_plm\n",
    "\n",
    "from utils import Mimic_ICD9_Processor, Mimic_ICD9_Triage_Processor, Mimic_Mortality_Processor\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from loguru import logger\n",
    "import json\n",
    "import itertools\n",
    "import torchmetrics.functional.classification as metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9664e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_prompt_model(ckpt_dir, params_dir,\n",
    "                              use_cuda = True):\n",
    "    \n",
    "    '''\n",
    "    Function to reload an already trained promptmodelclassifier. At moment this still requires data/task specific \n",
    "    manual template or verbalizers to be setup as they need to point to correct scripts.\n",
    "    \n",
    "    Args:\n",
    "        ckpt_dir: path to save promptmodel\n",
    "        params_dir: path to parameters of training - in newest pipeline will be same as checkpoints dir\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # load in saved paramters for the trained model\n",
    "    with open(f\"{params_dir}\") as f:\n",
    "        params = json.load(f)\n",
    "    # set up the parameters based on the training config\n",
    "    plm_type = params[\"model\"]\n",
    "    plm_name = params[\"model_name_or_path\"]\n",
    "    template_type = params[\"template_type\"]\n",
    "    template_id = params[\"template_id\"]\n",
    "    verbalizer_type = params[\"verbalizer_type\"]\n",
    "    verbalizer_id = params[\"verbalizer_id\"]\n",
    "    data_dir = params[\"data_dir\"]\n",
    "    dataset_name = params[\"dataset\"]\n",
    "    scripts_path = params[\"scripts_path\"]\n",
    "    init_from_vocab = params[\"init_from_vocab\"]\n",
    "    batch_size = params[\"batch_size\"]\n",
    "    tune_plm = params[\"tune_plm\"]\n",
    "    # set up datasets first    \n",
    "\n",
    "\n",
    "    dataset = {}\n",
    "    if dataset_name == \"icd9_50\":\n",
    "\n",
    "        logger.warning(f\"Using the following dataset: {dataset_name} \")\n",
    "        Processor = Mimic_ICD9_Processor\n",
    "        # update data_dir\n",
    "        data_dir = f\"{data_dir}/top_50_icd9\"\n",
    "\n",
    "        # get different splits\n",
    "        dataset['train'] = Processor().get_examples(data_dir = data_dir, mode = \"train\")\n",
    "\n",
    "        # the below class labels should align with the label encoder fitted to training data\n",
    "        # you will need to generate this class label text file first using the mimic processor with generate_class_labels flag to set true\n",
    "        # e.g. Processor().get_examples(data_dir = data_dir, mode = \"train\", generate_class_labels = True)[:10000]\n",
    "        class_labels =Processor().load_class_labels()\n",
    "        print(f\"number of classes: {len(class_labels)}\")\n",
    "        scriptsbase = f\"{scripts_path}/mimic_icd9_top50/\"\n",
    "        scriptformat = \"txt\"\n",
    "        max_seq_l = 480 # this should be specified according to the running GPU's capacity \n",
    "\n",
    "        batchsize_t = batch_size\n",
    "        batchsize_e = batch_size\n",
    "        gradient_accumulation_steps = 4\n",
    "        model_parallelize = False\n",
    "\n",
    "    elif dataset_name == \"icd9_triage\":\n",
    "        logger.warning(f\"Using the following dataset: {dataset_name} \")\n",
    "        Processor = Mimic_ICD9_Triage_Processor\n",
    "        # update data_dir\n",
    "        data_dir = f\"{data_dir}/triage\"\n",
    "\n",
    "        # get different splits\n",
    "        dataset['train'] = Processor().get_examples(data_dir = data_dir, mode = \"train\")\n",
    "\n",
    "        # the below class labels should align with the label encoder fitted to training data\n",
    "        # you will need to generate this class label text file first using the mimic processor with generate_class_labels flag to set true\n",
    "        # e.g. Processor().get_examples(data_dir = data_dir, mode = \"train\", generate_class_labels = True)[:10000]\n",
    "        class_labels =Processor().load_class_labels()\n",
    "        print(f\"number of classes: {len(class_labels)}\")\n",
    "        scriptsbase = f\"{scripts_path}/mimic_triage/\"\n",
    "        scriptformat = \"txt\"\n",
    "        max_seq_l = 480 # this should be specified according to the running GPU's capacity \n",
    "\n",
    "        batchsize_t = batch_size\n",
    "        batchsize_e = batch_size\n",
    "        gradient_accumulation_steps = 4\n",
    "        model_parallelize = False\n",
    "        \n",
    "    elif dataset_name == \"mortality\":\n",
    "        logger.warning(f\"Using the following dataset: {dataset_name} \")\n",
    "        Processor = Mimic_Mortality_Processor\n",
    "        # update data_dir\n",
    "        data_dir = \"../clinical-outcomes-data/mimic3-clinical-outcomes/mp/\"\n",
    "        \n",
    "        \n",
    "        dataset['train'] = Processor().get_examples(data_dir = data_dir, mode = \"train\", balance_data = False, class_weights=False, sampler_weights= False)[:1000]\n",
    "        \n",
    "        # the below class labels should align with the label encoder fitted to training data\n",
    "        # you will need to generate this class label text file first using the mimic processor with generate_class_labels flag to set true\n",
    "        # e.g. Processor().get_examples(data_dir = args.data_dir, mode = \"train\", generate_class_labels = True)[:10000]\n",
    "        class_labels = Processor().load_class_labels()\n",
    "        print(f\"class labels: {class_labels}\")\n",
    "        print(f\"number of classes: {len(class_labels)}\")\n",
    "        scriptsbase = f\"{scripts_path}/mimic_mortality/\"\n",
    "        scriptformat = \"txt\"\n",
    "        max_seq_l = 480 # this should be specified according to the running GPU's capacity \n",
    "        batchsize_t = batch_size\n",
    "        batchsize_e = batch_size\n",
    "        gradient_accumulation_steps = 4\n",
    "        model_parallelize = False\n",
    "\n",
    "\n",
    "    else:\n",
    "        #TODO implement icd9 triage and mimic readmission\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    \n",
    "    ######### set up the pretrained model etc ###########\n",
    "    \n",
    "    # initialise the pretrained language model\n",
    "    plm, tokenizer, model_config, WrapperClass = load_plm(plm_type, plm_name)    \n",
    "    \n",
    "    \n",
    "    # load the already trained prompt model, which will consist of a separate state_dict for the plm/template/verbalizer\n",
    "    loaded_model = torch.load(f\"{ckpt_dir}/best-checkpoint.ckpt\")\n",
    "    \n",
    "    \n",
    "    # now load the trained state_dict into the plm model if it was tuned during training\n",
    "    if tune_plm:\n",
    "        freeze_plm = False\n",
    "        print(\"PLM was tuned during training - loading the weights!\")\n",
    "#         plm.load_state_dict(loaded_model['plm'])\n",
    "    else:\n",
    "        freeze_plm=True\n",
    "        print(\"PLM was frozen during training to initializing from original pretrained weights!\")\n",
    "    \n",
    "    \n",
    "    # decide which template and verbalizer to use\n",
    "    if template_type == \"manual\":\n",
    "        print(f\"manual template selected, with id :{template_id}\")\n",
    "        mytemplate = ManualTemplate(tokenizer=tokenizer).from_file(f\"{scriptsbase}/manual_template.txt\", choice=template_id)\n",
    "\n",
    "    elif template_type == \"soft\":\n",
    "        print(f\"soft template selected, with id :{template_id}, will load template weights\")\n",
    "        mytemplate = SoftTemplate(model=plm, tokenizer=tokenizer, num_tokens=params['soft_token_num'], initialize_from_vocab=init_from_vocab).from_file(f\"{scriptsbase}/soft_template.txt\", choice=template_id)\n",
    "        # now load the state_dict from ckpt\n",
    "#         mytemplate.load_state_dict(loaded_model['template'])\n",
    "\n",
    "    elif template_type == \"mixed\":\n",
    "        print(f\"mixed template selected, with id :{template_id}, will load template weights\")\n",
    "        mytemplate = MixedTemplate(model=plm, tokenizer=tokenizer).from_file(f\"{scriptsbase}/mixed_template.txt\", choice=template_id)\n",
    "#         mytemplate.load_state_dict(loaded_model['template'])\n",
    "    # now set verbalizer\n",
    "    if verbalizer_type == \"manual\":\n",
    "        print(f\"manual verbalizer selected, with id :{verbalizer_id}\")\n",
    "        myverbalizer = ManualVerbalizer(tokenizer, classes=class_labels).from_file(f\"{scriptsbase}/manual_verbalizer.{scriptformat}\", choice=verbalizer_id)\n",
    "\n",
    "    elif verbalizer_type == \"soft\":\n",
    "        print(f\"soft verbalizer selected!\")\n",
    "        myverbalizer = SoftVerbalizer(tokenizer, plm, num_classes=len(class_labels))\n",
    "        # now load the state dict from saved checkpoint\n",
    "#         myverbalizer.load_state_dict(loaded_model['verbalizer'])\n",
    "        \n",
    "#     # now bring it all together into the prompt classification model\n",
    "\n",
    "    trained_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=freeze_plm)\n",
    "    # now load state dicts\n",
    "    trained_model.load_state_dict(state_dict = loaded_model)\n",
    "    \n",
    "    # send to cuda\n",
    "    if use_cuda:\n",
    "        print(\"using cuda!\")\n",
    "        trained_model =  trained_model.cuda()\n",
    "   \n",
    "    \n",
    "    \n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad3e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the number of parameters based on mixed tokenizer of varying number of soft tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60850117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft verbalizer selected!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbec8e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "459e9cf7",
   "metadata": {},
   "source": [
    "# finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0b794160",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = \"./logs/icd9_triage/emilyalsentzer/Bio_ClinicalBERT_tempmixed2_verbsoft0_fewshot_32/version_28-03-2022--10-29/checkpoints/\"\n",
    "params_dir = f\"{ckpt_dir}/hparams.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f0a7a88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-05 13:39:27.475 | WARNING  | __main__:load_trained_prompt_model:59 - Using the following dataset: icd9_triage \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train data\n",
      "data path provided was: ../mimic3-icd9-data/intermediary-data//triage/train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9559it [00:00, 14305.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLM was tuned during training - loading the weights!\n",
      "mixed template selected, with id :2, will load template weights\n",
      "soft verbalizer selected!\n"
     ]
    }
   ],
   "source": [
    "trained_model = load_trained_prompt_model(ckpt_dir, params_dir, use_cuda = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba603cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_model.prompt_model.plm.bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e37f730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_model.prompt_model.plm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbc5bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92ea5b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixedTemplate(\n",
       "  (raw_embedding): Embedding(28996, 768, padding_idx=0)\n",
       "  (soft_embedding): Embedding(4, 768)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ba169db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "626500"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in trained_model.verbalizer.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "441b4f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for getting trainable parameters of a model\n",
    "\n",
    "def get_n_trainable_params(model):    \n",
    "\n",
    "    \n",
    "    # all trainable\n",
    "    num_total_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    # split into the plm and classisifcation head\n",
    "    num_plm_trainable = sum(p.numel() for p in model.prompt_model.plm.parameters() if p.requires_grad)\n",
    "    \n",
    "    # template trainable\n",
    "    num_template_trainable = sum(p.numel() for p in model.template.soft_embedding.parameters() if p.requires_grad)\n",
    "    \n",
    "    # verbalizer trainable \n",
    "    num_verbalizer_trainable = sum(p.numel() for p in model.verbalizer.parameters() if p.requires_grad)\n",
    "    \n",
    "    # assert sum of the two = total\n",
    "    assert num_plm_trainable+num_template_trainable+num_verbalizer_trainable == num_total_trainable\n",
    "    \n",
    "    print(f\"Number of trainable parameters of PLM: {num_plm_trainable}\\n\")\n",
    "    print('#'*50)\n",
    "    print(f\"Number of trainable parameters of template: {num_template_trainable}\\n\")\n",
    "    print('#'*50)\n",
    "    print(f\"Number of trainable parameters of verbalizer: {num_verbalizer_trainable}\\n\")\n",
    "    print('#'*50)\n",
    "    print(f\"Total number of trainable parameters of whole model: {num_total_trainable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9bd786b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters of PLM: 108340804\n",
      "\n",
      "##################################################\n",
      "Number of trainable parameters of template: 3072\n",
      "\n",
      "##################################################\n",
      "Number of trainable parameters of verbalizer: 626500\n",
      "\n",
      "##################################################\n",
      "Total number of trainable parameters of whole model: 108970376\n"
     ]
    }
   ],
   "source": [
    "get_n_trainable_params(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e16c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9aadf6a",
   "metadata": {},
   "source": [
    "# frozen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a17340c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-05 14:39:02.568 | WARNING  | __main__:load_trained_prompt_model:59 - Using the following dataset: icd9_triage \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train data\n",
      "data path provided was: ../mimic3-icd9-data/intermediary-data//triage/train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9559it [00:00, 10399.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLM was frozen during training to initializing from original pretrained weights!\n",
      "mixed template selected, with id :2, will load template weights\n",
      "soft verbalizer selected!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ckpt_dir = \"./logs/icd9_triage/frozen_plm/emilyalsentzer/Bio_ClinicalBERT_tempmixed2_verbsoft0_fewshot_32/version_28-03-2022--10-56/checkpoints/\"\n",
    "params_dir = f\"{ckpt_dir}/hparams.txt\"\n",
    "\n",
    "trained_model = load_trained_prompt_model(ckpt_dir, params_dir, use_cuda = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "09c6f81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "626500"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in trained_model.verbalizer.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9841f3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.verbalizer.group_parameters_1[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "917f2ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters of PLM: 0\n",
      "\n",
      "##################################################\n",
      "Number of trainable parameters of template: 3072\n",
      "\n",
      "##################################################\n",
      "Number of trainable parameters of verbalizer: 626500\n",
      "\n",
      "##################################################\n",
      "Total number of trainable parameters of whole model: 629572\n"
     ]
    }
   ],
   "source": [
    "get_n_trainable_params(trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caca8935",
   "metadata": {},
   "source": [
    "# look at a manual verb option to confirm that there will be no tuneable verb parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63070ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 11:37:10.471 | WARNING  | __main__:load_trained_prompt_model:59 - Using the following dataset: icd9_triage \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train data\n",
      "data path provided was: ../mimic3-icd9-data/intermediary-data//triage/train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9559it [00:00, 14207.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLM was tuned during training - loading the weights!\n",
      "mixed template selected, with id :2, will load template weights\n",
      "manual verbalizer selected, with id :0\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = \"./logs/icd9_triage/emilyalsentzer/Bio_ClinicalBERT_tempmixed2_verbmanual0_full_100/version_23-03-2022--14-05/checkpoints/\"\n",
    "params_dir = f\"{ckpt_dir}/hparams.txt\"\n",
    "\n",
    "trained_model = load_trained_prompt_model(ckpt_dir, params_dir, use_cuda = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8cf9eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters of PLM: 108340804\n",
      "\n",
      "##################################################\n",
      "Number of trainable parameters of template: 3072\n",
      "\n",
      "##################################################\n",
      "Number of trainable parameters of verbalizer: 0\n",
      "\n",
      "##################################################\n",
      "Total number of trainable parameters of whole model: 108343876\n"
     ]
    }
   ],
   "source": [
    "get_n_trainable_params(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95667fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 11:38:30.912 | WARNING  | __main__:load_trained_prompt_model:59 - Using the following dataset: icd9_triage \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train data\n",
      "data path provided was: ../mimic3-icd9-data/intermediary-data//triage/train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9559it [00:00, 12738.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLM was frozen during training to initializing from original pretrained weights!\n",
      "mixed template selected, with id :2, will load template weights\n",
      "manual verbalizer selected, with id :0\n"
     ]
    }
   ],
   "source": [
    "# now frozen verb manual\n",
    "ckpt_dir = \"./logs/icd9_triage/frozen_plm/emilyalsentzer/Bio_ClinicalBERT_tempmixed2_verbmanual0_full_100/version_23-03-2022--13-00/checkpoints/\"\n",
    "params_dir = f\"{ckpt_dir}/hparams.txt\"\n",
    "\n",
    "trained_model = load_trained_prompt_model(ckpt_dir, params_dir, use_cuda = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "051bc7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters of PLM: 0\n",
      "\n",
      "##################################################\n",
      "Number of trainable parameters of template: 3072\n",
      "\n",
      "##################################################\n",
      "Number of trainable parameters of verbalizer: 0\n",
      "\n",
      "##################################################\n",
      "Total number of trainable parameters of whole model: 3072\n"
     ]
    }
   ],
   "source": [
    "get_n_trainable_params(trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8be524",
   "metadata": {},
   "source": [
    "# now want to look at how mixed template dictates number of parameters\n",
    "\n",
    "A mixed template consists of soft tokens that are initialized at weights from PLM, which will then be tuned, alongside manual tokens which will not be tuned. \n",
    "\n",
    "The dimension of these embeddings is typically going to be restricted to that of the PLM embeddings, e.g. 768 for most if not all bert based models.\n",
    "\n",
    "i.e. n_trainable_template_embeddings = 768*N_soft_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2721c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# instantiate the plm etc\n",
    "\n",
    "freeze_plm = True\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm('bert', 'emilyalsentzer/Bio_ClinicalBERT')   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3765651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertOnlyMLMHead(\n",
       "  (predictions): BertLMPredictionHead(\n",
       "    (transform): BertPredictionHeadTransform(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): Linear(in_features=768, out_features=28996, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plm.cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d1596e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cls'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_name = [n for n,c in plm.named_children()][-1]\n",
    "head_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "276ca79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in plm.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf2e8368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on mixed templated id: 4\n",
      "soft verbalizer selected!\n",
      "Number of trainable parameters of PLM: 0\n",
      "\n",
      "##################################################\n",
      "Number of trainable parameters of template: 1536\n",
      "\n",
      "##################################################\n",
      "Number of trainable parameters of verbalizer: 626500\n",
      "\n",
      "##################################################\n",
      "Total number of trainable parameters of whole model: 628036\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "temp_ids = [4]\n",
    "\n",
    "for temp_id in temp_ids:\n",
    "    print(f\"Working on mixed templated id: {temp_id}\")\n",
    "    \n",
    "    # mixed template \n",
    "    mytemplate = MixedTemplate(model=plm, tokenizer=tokenizer).from_file(f\"./scripts/mimic_triage/mixed_template.txt\", choice=temp_id)\n",
    "\n",
    "\n",
    "    print(f\"soft verbalizer selected!\")\n",
    "    myverbalizer = SoftVerbalizer(tokenizer, plm, num_classes=7)\n",
    "    \n",
    "\n",
    "prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=freeze_plm)\n",
    "\n",
    "get_n_trainable_params(prompt_model)\n",
    "\n",
    "print(\"#\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0a99fc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-0.4309, -0.3870, -0.4145,  ..., -0.5897, -0.5963, -0.5895],\n",
      "       requires_grad=True)\n",
      "torch.Size([28996])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2472, -0.0149, -0.0300,  ..., -0.0315, -0.0111,  0.0043],\n",
      "        [ 0.1146,  0.2637, -0.0006,  ..., -0.0473, -0.0150, -0.0258],\n",
      "        [-0.0057, -0.0517,  0.2008,  ..., -0.0372, -0.1174, -0.0789],\n",
      "        ...,\n",
      "        [ 0.0089, -0.0227, -0.0123,  ...,  0.3439,  0.0539,  0.0234],\n",
      "        [ 0.0407, -0.0772,  0.0116,  ...,  0.0249,  0.3978,  0.0338],\n",
      "        [ 0.0570, -0.0126, -0.1157,  ...,  0.0136,  0.0458,  0.2184]],\n",
      "       requires_grad=True)\n",
      "torch.Size([768, 768])\n",
      "Parameter containing:\n",
      "tensor([ 1.6920e-02,  8.6847e-02,  6.4819e-02,  4.8258e-02,  3.1324e-02,\n",
      "         3.8392e-02,  9.7983e-03,  2.8769e-02,  3.9382e-02,  2.7226e-02,\n",
      "         3.8215e-02,  3.8464e-02,  4.5411e-02,  3.8577e-02,  5.3082e-02,\n",
      "        -1.9338e-02,  3.6245e-02,  8.1958e-02,  4.4114e-02,  8.4626e-02,\n",
      "         1.1240e-01,  8.1564e-02,  2.5398e-02,  1.4152e-02,  4.0458e-02,\n",
      "         2.6787e-02,  3.2342e-02,  4.3955e-02,  3.6961e-02,  3.8028e-02,\n",
      "         6.0274e-02, -3.6334e-02,  5.2809e-02,  3.2633e-02,  8.2999e-02,\n",
      "         5.5116e-02,  3.4200e-02,  3.2404e-02, -4.2751e-02,  3.9814e-02,\n",
      "         6.9006e-02,  3.8158e-02,  2.7113e-02,  3.0713e-02,  2.3202e-02,\n",
      "         1.9184e-02,  3.1827e-02,  3.2232e-02,  5.4492e-02,  2.9954e-02,\n",
      "         2.5678e-02,  3.6673e-02,  2.9236e-02,  4.4605e-02,  4.4676e-02,\n",
      "         3.1217e-02,  3.6420e-02,  3.3358e-02,  3.7168e-02,  3.5766e-02,\n",
      "         4.6127e-02,  6.5827e-02,  4.2323e-02,  5.0637e-02,  4.4924e-03,\n",
      "         4.4566e-02,  3.6932e-02,  2.5319e-02, -5.5384e-02,  3.9912e-02,\n",
      "         3.4207e-02,  6.0261e-02,  4.7715e-02,  1.2039e-02,  2.9396e-02,\n",
      "         4.5187e-02,  5.7007e-02,  3.7048e-02,  4.0138e-02,  1.2771e-02,\n",
      "         8.3350e-02,  4.1908e-02,  4.6994e-02,  1.1806e-01, -4.2543e-04,\n",
      "         3.2810e-02,  3.5557e-02,  3.3126e-02,  7.1949e-02,  3.2725e-02,\n",
      "         5.0423e-02,  5.0912e-02,  3.4161e-02,  5.1742e-02,  2.6180e-02,\n",
      "         2.9987e-02, -5.1040e-03,  5.8268e-02,  8.7598e-02,  4.4297e-02,\n",
      "         1.5230e-04,  2.9481e-02,  2.8568e-02,  4.3038e-02, -2.0066e-02,\n",
      "         3.8575e-02,  3.6808e-02,  2.7188e-02,  3.5340e-02,  5.4547e-02,\n",
      "         5.0044e-02,  2.1018e-02,  3.4629e-02, -7.6771e-02,  3.3887e-02,\n",
      "         2.6292e-02,  5.7552e-02,  3.7824e-02,  1.5239e-01,  2.9863e-02,\n",
      "         2.5301e-02,  5.1639e-02,  4.7652e-02,  3.0185e-02,  6.1130e-02,\n",
      "         6.8922e-02,  2.2722e-02,  2.3580e-02,  4.3234e-02,  3.6810e-02,\n",
      "         4.5234e-02,  6.2944e-02,  8.5121e-02,  3.4035e-02,  5.7494e-02,\n",
      "         4.4844e-02, -1.7447e-02,  2.6992e-02,  6.7025e-02,  3.8319e-02,\n",
      "         1.1525e-02,  4.4428e-02,  4.9251e-02, -7.9443e-03,  1.8636e-02,\n",
      "         3.5735e-02,  6.7884e-02,  3.2873e-02,  4.3121e-02,  3.3084e-02,\n",
      "         3.5227e-02,  3.0590e-02,  4.2572e-02,  6.2777e-02,  4.3066e-02,\n",
      "         4.8572e-02,  2.9915e-02,  5.4016e-02,  4.7047e-02,  3.6308e-02,\n",
      "         3.3554e-02,  4.1288e-02, -1.1616e-01,  2.4861e-02,  3.3316e-02,\n",
      "         3.4995e-02,  4.5558e-02,  6.8511e-02,  3.0373e-02,  2.9514e-02,\n",
      "         3.4399e-02, -5.7422e-03,  1.1301e-01,  2.4359e-02,  3.2913e-02,\n",
      "         3.4097e-02,  1.9993e-02,  5.9643e-02,  1.9408e-02,  2.5449e-02,\n",
      "         3.2398e-02,  2.5241e-02,  4.0876e-02,  2.0983e-02,  8.8250e-02,\n",
      "         2.2298e-02,  3.6985e-02,  4.6189e-02,  5.6211e-02,  6.7460e-02,\n",
      "         3.2571e-02, -1.0505e-02,  3.0112e-02,  3.4410e-02,  9.8887e-02,\n",
      "         3.6234e-02,  4.7742e-02,  4.0648e-04,  3.5967e-02,  5.7249e-02,\n",
      "         1.7418e-02,  4.1032e-02,  2.9348e-02,  5.5493e-02,  3.9923e-02,\n",
      "         3.5245e-02,  5.5587e-02,  3.7301e-02,  4.1882e-02,  3.0082e-02,\n",
      "         3.5573e-02,  2.3965e-02,  2.9328e-02,  3.5127e-02,  2.7188e-02,\n",
      "         1.9408e-02,  4.3238e-02,  3.5041e-02,  6.2237e-02,  5.3912e-02,\n",
      "         1.3049e-01,  1.2962e-01,  4.5404e-02,  3.6112e-02,  2.4829e-02,\n",
      "         1.1614e-01,  1.3465e-02,  4.1650e-02,  8.4499e-02, -4.1638e-02,\n",
      "         8.6532e-02,  4.0618e-02,  3.4218e-02,  4.9451e-02,  4.7473e-02,\n",
      "         4.3817e-02, -4.8590e-02,  3.5224e-02,  4.1844e-02,  4.9478e-02,\n",
      "         2.4531e-02,  3.5184e-02,  8.8173e-02,  3.6715e-02,  3.9066e-02,\n",
      "         4.3481e-02,  3.7409e-02,  4.6853e-02,  3.4762e-02,  5.1969e-02,\n",
      "         2.9870e-02,  4.5199e-02,  3.2390e-02,  2.8300e-02,  5.3961e-02,\n",
      "         2.8385e-02,  5.2370e-02, -3.1365e-02,  5.8121e-02,  4.0932e-02,\n",
      "        -1.5858e-02,  4.5994e-02,  3.6726e-02,  2.7251e-02,  7.2328e-02,\n",
      "         3.5665e-02,  2.9288e-02,  2.6215e-02, -8.1190e-03,  3.4497e-02,\n",
      "         2.1833e-02,  4.1149e-02,  5.2994e-02,  4.0489e-02,  3.2292e-02,\n",
      "         4.6756e-02,  2.7626e-02,  2.9272e-02,  5.6208e-02,  4.2239e-02,\n",
      "         1.8418e-02,  4.0960e-02,  3.3892e-02,  2.6719e-02,  4.8190e-02,\n",
      "         2.1538e-02,  4.7981e-02,  3.9168e-02,  2.2772e-02,  4.2984e-02,\n",
      "         8.5398e-02,  2.5104e-02,  8.2281e-02,  1.0378e-01,  4.5250e-02,\n",
      "         2.9702e-02,  3.2016e-02,  1.3400e-02,  3.9474e-02,  2.8408e-02,\n",
      "         4.8310e-02,  2.0110e-02,  3.3513e-02,  1.7896e-02, -2.0542e-02,\n",
      "         2.4718e-02,  3.4849e-02,  3.0654e-02,  6.6199e-02,  2.3568e-02,\n",
      "         4.4349e-02,  4.9084e-02,  5.1901e-02,  2.7962e-02,  4.3286e-02,\n",
      "         5.4147e-02,  2.6176e-02,  3.4542e-02,  4.4564e-02,  4.0589e-02,\n",
      "         4.6173e-02,  8.6253e-02,  2.0612e-02,  6.6563e-02,  4.6486e-02,\n",
      "         4.3819e-02,  2.9948e-02,  2.5010e-02,  4.8687e-02,  2.8211e-02,\n",
      "         3.7415e-02,  4.7944e-02,  7.2990e-02,  3.8096e-02,  8.1552e-02,\n",
      "         3.0028e-02,  6.0264e-02,  3.7389e-02,  6.1504e-02,  1.5748e-02,\n",
      "         1.8111e-02,  1.0826e-01,  5.0103e-02,  5.3024e-02,  2.6765e-02,\n",
      "         3.2302e-02,  3.8243e-02,  3.4599e-02,  3.5956e-02,  5.0336e-02,\n",
      "         3.6624e-02,  7.3996e-02,  4.3796e-02,  2.3361e-02,  4.2312e-02,\n",
      "         3.0875e-02,  3.6105e-02,  4.1800e-02,  1.3041e-02,  2.2409e-02,\n",
      "         7.7273e-02,  3.8888e-02,  3.9454e-02,  7.8015e-02,  4.1913e-02,\n",
      "         2.3425e-02,  2.2255e-02,  4.3250e-02,  9.2089e-02,  1.0301e-01,\n",
      "         3.7956e-02,  3.4726e-02,  6.5435e-02,  4.7532e-02,  3.9921e-02,\n",
      "         7.9323e-02,  3.9405e-02,  8.2994e-02,  4.7698e-02,  2.2827e-02,\n",
      "         1.1298e-01,  4.2453e-02,  2.2087e-02,  3.9623e-02,  7.4139e-02,\n",
      "         3.5348e-02,  4.0351e-02,  3.5449e-02,  4.0412e-02,  3.7407e-02,\n",
      "         7.9047e-02,  3.4428e-02,  3.8192e-02,  3.4189e-02,  3.1766e-02,\n",
      "         7.8722e-02,  2.6218e-02,  4.9760e-02,  5.4598e-02,  4.1648e-02,\n",
      "         8.1813e-02,  5.7450e-02,  5.0912e-02,  5.9349e-03,  4.6921e-02,\n",
      "         3.2622e-02,  5.0982e-02,  2.6760e-02,  3.9222e-02,  3.1840e-02,\n",
      "         2.0787e-02,  3.8571e-02,  5.4562e-02,  4.1489e-02,  3.5227e-02,\n",
      "         4.5085e-02,  4.9419e-02,  7.5441e-02,  1.3105e-01,  8.5052e-02,\n",
      "         4.6666e-02,  3.0217e-02,  5.3543e-02,  5.7492e-02,  3.9437e-02,\n",
      "         4.0892e-02,  4.2063e-02, -4.7332e-02,  5.3139e-02,  3.5798e-02,\n",
      "         4.8284e-02,  3.4129e-02,  1.2803e-02,  9.6008e-02,  1.7361e-02,\n",
      "         2.1937e-02,  4.0314e-02,  2.9818e-02,  3.6334e-02,  5.1518e-02,\n",
      "         3.5084e-02,  1.1717e-02,  4.0647e-02,  2.1139e-02,  8.3497e-02,\n",
      "         3.4596e-02,  5.4961e-02,  1.7523e-02,  3.2575e-02,  5.9479e-02,\n",
      "         9.7192e-02, -7.6530e-02,  3.0748e-02,  3.1692e-02,  4.8351e-02,\n",
      "         5.2899e-02,  1.0546e-01,  3.1578e-02,  1.2281e-01,  8.4012e-02,\n",
      "         4.3004e-02,  3.4581e-02,  3.5091e-02,  2.6685e-02,  5.2785e-02,\n",
      "         4.0032e-02,  2.1873e-02,  2.5435e-02,  2.8885e-02,  3.6413e-02,\n",
      "         4.9343e-02,  6.5060e-02,  4.1614e-02,  5.2295e-02,  2.9768e-02,\n",
      "         3.6230e-02,  1.6937e+00,  3.7112e-02,  5.1045e-02,  3.3838e-02,\n",
      "         3.3835e-02,  8.8975e-02,  1.2322e-01,  2.1197e-02,  8.1698e-02,\n",
      "         4.7252e-02,  5.2702e-02,  5.8279e-02,  2.7906e-02,  1.5897e-01,\n",
      "         4.2010e-02,  3.5939e-02,  5.5186e-02,  2.1300e-02,  3.7711e-02,\n",
      "         2.9810e-02,  3.5906e-02,  3.5465e-02,  3.8022e-02,  8.8402e-02,\n",
      "         3.7840e-02,  7.1490e-02,  4.0724e-02,  4.3529e-02,  5.5832e-02,\n",
      "         7.2928e-02,  4.4517e-02,  4.5919e-02,  3.6071e-03,  1.0837e-01,\n",
      "         2.5416e-02,  6.3141e-02,  2.4004e-02,  1.8659e-02,  2.5078e-02,\n",
      "         1.1845e-01,  4.6982e-02,  3.8094e-02, -5.1360e-02,  5.6536e-02,\n",
      "         3.3378e-02,  4.6654e-02,  4.7710e-02,  3.4694e-02,  2.6221e-02,\n",
      "         4.3316e-02,  3.2434e-02,  1.6612e-02,  6.4295e-02,  4.8648e-02,\n",
      "         3.8450e-02,  3.0717e-02,  2.6898e-02,  4.5302e-02,  3.1842e-02,\n",
      "         2.0684e-02,  4.3067e-02,  8.0662e-02,  3.1993e-02,  2.4427e-02,\n",
      "         3.3847e-02,  3.1770e-02,  1.8351e-02,  5.7270e-02,  2.2943e-02,\n",
      "         3.9429e-02,  2.7993e-02,  4.9311e-02,  6.2847e-02,  4.2802e-02,\n",
      "         1.3509e-01,  2.2216e-02,  3.5962e-02,  6.5648e-02,  3.7933e-02,\n",
      "         7.7508e-02,  3.1816e-02, -3.6574e-02,  3.1217e-02,  7.2300e-02,\n",
      "         4.4463e-02,  3.4894e-02, -3.4795e-02,  1.4202e-02,  4.7153e-02,\n",
      "         5.3864e-02,  4.8882e-02,  1.2920e-02,  5.8314e-02,  3.2612e-02,\n",
      "         2.4868e-02,  8.9324e-02,  3.8661e-02,  4.3235e-02,  2.2946e-02,\n",
      "         3.0607e-02,  3.3370e-02,  2.6166e-02,  2.2929e-02,  3.2182e-02,\n",
      "         1.1845e-01,  2.4374e-02,  4.0644e-02,  5.3156e-02,  5.2724e-02,\n",
      "         2.7829e-02,  3.2711e-02,  5.6563e-02,  2.4457e-02,  4.2197e-02,\n",
      "         4.0065e-02,  2.7717e-02,  2.6170e-02,  8.0638e-03,  4.1045e-02,\n",
      "         3.5474e-02,  3.1824e-02,  4.8778e-02,  5.4734e-02,  3.4857e-02,\n",
      "         4.6260e-02,  5.9562e-02,  2.4928e-02,  4.1794e-02,  5.1188e-02,\n",
      "         3.3194e-02,  3.1697e-02,  6.8040e-02,  2.3790e-02,  2.8879e-02,\n",
      "         3.6165e-02,  6.6883e-02,  2.4420e-02,  3.7019e-02,  4.1306e-02,\n",
      "         4.6715e-02,  4.7522e-02,  4.8107e-02,  3.2385e-02,  5.3959e-02,\n",
      "         3.6537e-02,  3.4700e-02,  2.6922e-02,  3.5531e-02,  4.5719e-02,\n",
      "         4.0741e-02,  3.6898e-02,  3.0770e-02,  1.8801e-02,  2.8610e-02,\n",
      "         4.0013e-02,  3.5884e-02,  6.9295e-03,  3.7700e-02,  3.5423e-02,\n",
      "         5.2116e-02,  2.9404e-02,  3.7266e-02,  4.2709e-02,  5.0853e-02,\n",
      "         8.5231e-02,  3.8512e-02,  4.6348e-02,  3.8980e-02,  3.4186e-02,\n",
      "         2.0871e-02, -1.1316e-02,  4.1619e-02,  4.2561e-02,  1.7946e-02,\n",
      "         2.4115e-02,  7.0282e-02,  4.0522e-02, -8.9482e-03,  2.9002e-02,\n",
      "         3.2229e-02,  4.5336e-02,  2.6914e-02,  2.6522e-02,  4.0078e-02,\n",
      "         2.9900e-02,  3.0428e-02,  4.2887e-02,  5.2362e-02,  6.0938e-02,\n",
      "         3.3640e-02,  4.8597e-02, -5.4060e-02,  2.7783e-02,  5.8180e-02,\n",
      "         6.4013e-02,  6.4912e-02,  1.4809e-02,  4.4534e-02,  4.5003e-02,\n",
      "         6.1310e-02,  5.8870e-02,  7.1767e-02,  8.4761e-02,  5.6933e-02,\n",
      "         5.2445e-02,  5.0347e-02,  5.6091e-02,  5.9018e-02,  4.6029e-02,\n",
      "         3.0710e-02,  3.7534e-02,  6.1099e-02,  2.5145e-02,  4.8335e-02,\n",
      "         3.6129e-02, -6.0895e-02,  4.3183e-02,  2.8785e-02,  3.0580e-02,\n",
      "         4.5236e-02,  4.8893e-02,  6.8761e-02,  3.0353e-02,  3.2755e-02,\n",
      "         2.6042e-02,  3.4025e-02, -7.4684e-03,  4.0050e-02,  4.4658e-02,\n",
      "         5.5283e-02,  2.4834e-02,  3.1771e-02,  4.4873e-02,  3.5689e-02,\n",
      "         3.3038e-02,  3.8265e-02,  2.7738e-02,  3.0114e-02,  3.2765e-02,\n",
      "         1.3108e-02,  9.1867e-02,  6.3517e-02,  2.7313e-02,  2.7191e-02,\n",
      "         4.9239e-02,  4.6895e-02,  4.2808e-02,  5.8148e-02,  4.4929e-02,\n",
      "        -1.1029e-01,  2.7504e-02,  2.8065e-02,  4.6359e-02,  3.4542e-02,\n",
      "         5.7760e-02,  3.4240e-02,  4.4560e-02,  3.2699e-02,  7.4266e-02,\n",
      "         5.3711e-02,  3.5609e-02,  4.4265e-02,  4.2928e-02,  3.3883e-02,\n",
      "         3.6639e-02,  7.4941e-02, -2.0083e-02,  3.1615e-02,  5.4549e-02,\n",
      "         2.2561e-02,  3.9860e-02,  3.6338e-02,  3.4045e-02, -1.3757e-02,\n",
      "         5.1814e-02,  2.6363e-02,  4.1178e-02,  6.3267e-02,  5.5961e-02,\n",
      "         1.1501e-01,  4.0048e-02,  4.2766e-02,  5.4254e-02,  1.8161e-02,\n",
      "         2.3855e-02,  1.0277e-01,  3.6711e-02,  3.9486e-02,  4.6032e-02,\n",
      "         4.2865e-02,  4.1236e-02,  4.4166e-02], requires_grad=True)\n",
      "torch.Size([768])\n",
      "Parameter containing:\n",
      "tensor([2.2651, 2.5379, 2.3919, 2.4304, 3.1542, 2.3489, 0.2924, 0.9216, 2.4980,\n",
      "        2.3091, 2.5318, 2.6145, 2.5558, 2.4121, 2.5636, 0.3061, 2.3744, 2.3549,\n",
      "        2.5761, 2.5638, 2.8254, 2.3834, 2.2494, 2.4517, 2.5850, 2.5571, 2.3071,\n",
      "        2.3424, 2.4532, 2.3801, 2.3947, 0.6354, 2.4118, 2.5325, 2.3523, 2.4071,\n",
      "        2.3172, 2.5029, 0.5465, 2.4113, 2.5840, 2.5097, 2.4072, 2.4928, 2.4107,\n",
      "        2.2661, 2.4373, 2.2901, 2.3913, 2.3139, 2.4454, 2.3689, 2.3148, 2.3166,\n",
      "        2.5507, 0.1565, 2.3604, 2.6008, 2.5110, 2.3326, 2.4804, 2.5888, 2.5540,\n",
      "        2.3005, 0.5901, 2.6538, 2.4719, 2.3070, 0.6707, 2.4851, 2.4571, 2.6135,\n",
      "        2.4781, 2.5019, 2.5422, 2.4083, 2.4473, 2.4151, 2.6400, 2.4790, 2.7664,\n",
      "        2.5202, 2.4018, 2.5581, 0.4989, 2.2920, 2.5829, 2.3735, 2.5614, 2.2488,\n",
      "        2.4076, 2.5656, 2.3800, 2.6364, 2.4243, 2.4810, 0.9831, 2.3214, 2.5673,\n",
      "        2.4977, 2.3266, 2.3339, 2.3480, 2.7671, 0.1682, 2.4453, 2.4920, 2.3040,\n",
      "        2.5053, 2.5717, 2.3730, 2.6326, 2.5001, 0.8513, 2.3566, 2.4307, 2.3839,\n",
      "        2.4596, 2.4526, 2.4525, 2.3543, 2.4037, 2.3472, 2.4163, 2.4522, 2.3430,\n",
      "        2.3651, 2.4299, 2.4078, 2.5270, 2.5122, 2.3269, 2.6116, 2.5035, 2.5738,\n",
      "        2.4548, 0.5772, 2.2725, 2.8297, 2.3385, 0.1393, 2.3959, 2.3649, 2.4124,\n",
      "        2.3381, 2.5155, 2.7204, 2.4070, 2.4801, 2.3357, 2.5564, 2.4569, 2.3081,\n",
      "        2.5283, 2.4272, 2.4745, 2.6183, 2.4445, 2.5076, 2.5046, 2.3971, 2.3270,\n",
      "        0.7042, 2.4670, 2.3925, 2.2886, 2.5528, 1.1693, 2.4052, 2.4420, 2.5609,\n",
      "        0.3335, 2.5132, 2.6244, 2.5272, 2.4382, 2.3798, 2.3506, 2.3236, 2.4824,\n",
      "        2.2743, 2.4313, 2.4211, 2.3607, 2.5144, 2.4780, 2.3359, 2.6376, 2.5215,\n",
      "        2.5808, 2.4255, 0.1583, 2.6215, 2.3929, 2.3964, 2.2977, 2.4846, 0.6011,\n",
      "        2.4086, 2.4892, 2.4366, 2.7318, 2.5853, 2.6794, 2.2871, 2.4508, 2.4905,\n",
      "        2.4963, 2.3351, 2.4852, 2.4736, 2.2502, 2.4881, 2.4829, 2.3701, 2.4851,\n",
      "        2.5171, 2.3960, 2.4170, 2.4818, 2.4150, 2.4329, 2.5314, 2.4633, 2.2594,\n",
      "        2.5452, 2.4899, 2.3676, 2.6615, 0.9685, 2.6018, 2.4272, 2.3927, 2.4337,\n",
      "        2.3051, 2.5306, 0.4099, 2.6572, 2.2663, 2.4249, 2.3894, 2.2966, 2.4739,\n",
      "        2.3577, 2.3891, 2.3337, 2.4298, 2.4319, 2.5054, 2.4181, 2.3428, 2.4425,\n",
      "        2.5441, 2.6444, 2.4550, 2.5560, 2.6546, 0.4673, 2.8388, 2.4852, 0.6945,\n",
      "        2.3883, 2.5919, 2.3075, 2.3982, 2.4557, 2.3700, 2.3477, 0.2020, 2.4735,\n",
      "        2.4668, 2.4181, 2.5507, 2.3098, 2.5065, 2.3408, 2.4068, 2.3196, 2.4946,\n",
      "        2.6558, 2.5249, 2.5171, 2.7397, 2.5135, 2.3895, 2.3907, 2.5796, 2.4004,\n",
      "        2.4364, 2.3541, 3.4594, 2.3210, 2.6004, 2.9118, 2.2557, 2.3768, 2.2233,\n",
      "        0.3084, 2.4595, 2.4636, 2.5880, 2.2358, 2.3393, 2.6844, 0.2635, 2.2553,\n",
      "        2.3576, 2.4533, 2.5145, 2.3018, 2.3496, 2.4881, 2.5523, 0.3122, 2.4143,\n",
      "        2.6199, 2.5988, 2.3427, 2.4872, 2.3587, 2.3065, 2.6377, 2.2827, 2.4641,\n",
      "        2.2770, 2.6057, 2.4817, 2.3443, 2.4537, 2.5303, 2.3779, 2.4071, 2.5249,\n",
      "        2.3669, 2.7555, 2.6122, 2.2964, 2.3634, 2.3505, 2.5041, 0.8218, 2.4806,\n",
      "        2.4266, 2.4517, 2.3722, 2.4271, 2.4845, 2.2731, 2.3760, 2.2805, 2.4630,\n",
      "        2.5442, 2.5554, 2.2883, 2.4894, 2.3915, 2.4908, 2.2014, 2.3341, 2.4171,\n",
      "        2.6671, 2.3852, 2.5889, 2.5493, 2.3179, 2.4541, 2.3231, 2.3912, 2.3575,\n",
      "        2.3993, 2.6008, 2.2250, 2.4344, 2.4620, 2.4836, 2.6437, 2.4844, 2.4584,\n",
      "        2.5315, 0.4990, 2.4789, 2.3879, 2.4976, 2.4367, 2.5635, 2.4382, 2.3940,\n",
      "        2.4639, 2.6315, 2.2588, 2.4796, 2.3823, 2.3084, 2.4472, 2.5176, 2.6152,\n",
      "        2.5145, 2.4920, 2.3309, 2.3930, 2.9608, 2.6011, 2.4704, 2.4201, 2.5525,\n",
      "        2.2884, 2.5134, 2.3065, 2.3861, 2.4847, 2.4508, 2.4421, 2.6787, 2.5102,\n",
      "        2.2660, 2.5410, 2.5324, 2.6654, 2.6015, 2.4742, 2.3895, 2.5651, 2.3879,\n",
      "        2.4706, 2.4171, 2.3014, 2.3833, 0.5425, 2.4250, 2.4776, 2.3560, 2.4964,\n",
      "        2.3962, 2.5585, 0.4329, 2.4842, 2.6842, 2.4595, 2.4156, 2.5178, 2.4656,\n",
      "        2.3764, 2.2854, 2.5538, 2.5655, 2.3470, 2.4145, 2.3536, 2.2872, 2.4813,\n",
      "        2.4482, 0.6242, 2.3034, 2.5784, 2.3201, 2.5216, 2.4998, 2.2495, 2.4542,\n",
      "        2.5747, 2.4103, 2.5661, 2.4495, 2.6048, 2.4539, 2.4805, 2.3538, 2.2867,\n",
      "        2.3865, 2.4096, 2.5288, 2.5864, 2.5251, 2.4926, 2.3172, 2.4065, 0.0686,\n",
      "        2.3846, 2.3878, 2.5316, 2.3539, 2.4422, 2.5310, 2.3355, 2.4422, 2.3848,\n",
      "        2.5528, 2.4187, 2.3338, 2.4436, 2.6639, 2.4253, 2.5452, 2.4479, 2.4482,\n",
      "        2.3748, 2.4254, 2.4774, 2.3707, 2.4343, 2.3067, 2.4287, 2.3852, 2.3720,\n",
      "        2.4125, 2.4197, 2.4464, 2.4460, 2.3139, 2.4690, 2.3017, 2.4908, 2.5107,\n",
      "        2.3861, 2.5140, 2.5347, 2.3722, 2.3311, 0.1099, 2.4568, 2.6119, 2.4573,\n",
      "        2.4684, 2.4422, 2.2781, 2.5559, 2.4881, 2.5185, 2.5814, 2.5267, 2.3129,\n",
      "        2.3953, 2.5551, 2.4398, 2.4025, 2.4001, 2.3835, 2.5836, 2.3943, 2.3909,\n",
      "        2.5364, 2.4177, 2.3375, 2.8382, 2.5731, 2.5444, 2.2003, 2.3967, 2.5690,\n",
      "        2.5364, 2.5205, 2.5198, 2.4309, 2.4366, 2.2059, 2.4522, 2.3348, 2.1974,\n",
      "        2.3525, 2.4468, 2.3073, 2.3958, 0.0898, 2.5702, 2.3900, 2.3885, 2.5041,\n",
      "        2.5221, 2.5941, 2.4553, 2.2472, 2.3098, 2.3224, 2.4533, 2.3131, 2.2896,\n",
      "        2.3318, 2.3873, 2.5113, 2.4740, 2.5641, 2.5110, 2.5224, 2.7833, 2.6486,\n",
      "        2.3363, 2.4907, 2.3931, 2.3394, 2.6417, 2.3330, 2.3924, 2.6068, 2.3668,\n",
      "        2.3923, 2.5631, 2.3224, 2.5079, 2.4764, 2.3834, 2.4294, 2.4211, 2.4348,\n",
      "        2.3552, 2.4028, 2.3021, 2.2823, 2.4024, 2.4821, 2.5725, 2.4335, 2.5052,\n",
      "        2.4004, 2.2910, 2.4126, 2.5290, 2.4738, 2.5385, 2.5123, 2.3767, 2.4824,\n",
      "        2.3955, 2.3876, 2.5411, 2.3852, 2.4008, 2.5063, 2.4083, 2.3808, 2.5161,\n",
      "        2.4828, 2.4862, 2.3670, 2.3662, 2.5850, 2.4538, 2.3075, 2.2970, 2.5262,\n",
      "        2.3665, 2.4480, 2.5234, 2.5547, 2.2948, 2.3772, 2.3991, 0.2956, 2.5596,\n",
      "        2.4905, 2.2520, 2.3795, 2.5743, 2.4239, 0.4500, 2.4190, 2.3807, 2.5282,\n",
      "        2.3553, 2.3765, 2.5245, 2.3965, 2.6186, 2.3923, 2.6289, 2.4325, 2.3787,\n",
      "        2.5426, 0.5048, 2.6211, 2.6788, 2.5083, 2.5469, 2.2845, 2.5051, 2.5943,\n",
      "        2.8331, 2.4334, 2.4406, 2.7229, 2.4993, 2.4358, 2.3521, 2.4151, 2.6358,\n",
      "        2.4637, 2.5233, 2.3946, 2.5449, 2.3301, 2.5475, 2.3841, 0.7602, 2.3279,\n",
      "        2.3497, 2.6450, 2.3953, 2.5211, 2.4515, 2.3023, 2.4605, 2.4197, 2.4426,\n",
      "        0.3823, 2.3703, 2.4779, 2.4282, 2.4519, 2.2462, 2.3854, 2.5425, 2.4412,\n",
      "        2.4380, 2.5682, 2.4094, 2.5367, 2.7025, 2.3690, 2.8804, 2.3804, 2.3558,\n",
      "        2.5367, 2.5567, 2.4108, 2.5618, 2.4978, 0.3016, 2.5893, 2.3964, 2.4684,\n",
      "        2.3690, 2.5606, 2.3724, 2.4973, 2.3927, 2.4734, 2.3925, 2.5256, 2.3830,\n",
      "        2.4256, 2.4044, 2.4551, 2.4827, 0.4258, 2.4953, 2.5571, 2.4046, 2.3928,\n",
      "        2.3756, 2.3878, 0.2435, 2.4541, 2.4510, 2.3806, 2.4436, 2.3793, 2.4485,\n",
      "        2.4598, 2.4655, 2.2980, 2.4675, 2.7041, 2.4998, 2.4932, 2.5175, 2.4765,\n",
      "        2.4240, 2.4663, 2.3695], requires_grad=True)\n",
      "torch.Size([768])\n",
      "Parameter containing:\n",
      "tensor([-2.9018e-01,  3.1163e-01, -3.2427e-01, -1.5150e-01,  4.8210e-01,\n",
      "        -2.3926e-01,  1.0284e-01,  3.7846e-02,  2.1407e-01, -5.4488e-01,\n",
      "         2.9657e-01,  3.4409e-01,  2.1367e-01, -2.1184e-01,  3.2799e-01,\n",
      "         2.1031e-01,  4.9619e-02,  2.6846e-01,  4.7098e-01,  3.9789e-01,\n",
      "         2.4207e-01,  1.0386e-01, -1.3712e-01, -3.0366e-01,  3.4476e-02,\n",
      "         2.4520e-01, -1.3374e-01, -2.1228e-01,  4.6911e-01,  3.4394e-01,\n",
      "         2.6769e-01, -8.4989e-02,  3.4923e-01, -4.2349e-02,  2.6219e-01,\n",
      "         3.1831e-02, -1.4034e-01,  5.7423e-02,  2.7597e-01,  2.6728e-01,\n",
      "         3.5336e-01,  2.5161e-01, -4.2017e-01,  2.3487e-01, -2.2792e-01,\n",
      "        -1.2340e-01, -1.7135e-01,  4.1609e-02,  3.5646e-01,  6.2537e-02,\n",
      "        -3.1900e-01, -1.7748e-01, -4.2961e-03, -1.1698e-01,  5.3656e-02,\n",
      "         9.4525e-02,  2.8089e-01,  4.1961e-01, -1.0520e-01, -2.5807e-01,\n",
      "         4.4160e-01,  2.5621e-01,  1.7660e-01, -2.8702e-01,  4.1319e-01,\n",
      "         1.1209e-01, -2.4859e-01, -2.2579e-01,  4.5720e-01,  3.6642e-01,\n",
      "        -5.9711e-02, -2.1936e-01,  2.9176e-01, -2.4857e-01,  3.7163e-01,\n",
      "        -1.4038e-01,  1.9879e-01, -3.1563e-01,  3.7328e-01, -5.9360e-01,\n",
      "         2.3353e-01,  3.3482e-01,  1.2505e-01,  3.3423e-01, -7.5713e-02,\n",
      "        -1.0497e-01,  2.9344e-01,  5.2131e-02,  3.7614e-01, -3.5553e-01,\n",
      "        -2.3010e-01, -2.3885e-01, -7.0111e-02,  4.6658e-01,  1.2338e-01,\n",
      "         2.0826e-01,  4.2316e-01, -1.3763e-02,  2.7608e-01, -1.1219e-01,\n",
      "        -3.4103e-01,  2.9180e-01, -2.1351e-01,  4.1371e-01,  3.5458e-02,\n",
      "         1.8301e-01, -7.9573e-02, -2.6672e-01,  2.1890e-01,  2.3712e-01,\n",
      "        -9.4469e-04,  1.9629e-01,  4.0692e-01,  2.2285e-01, -9.1907e-02,\n",
      "        -2.3227e-01,  4.0975e-01,  1.9448e-01,  2.1693e-01,  2.1348e-01,\n",
      "        -2.9180e-01, -2.7144e-01, -1.2118e-01,  2.0075e-01,  2.0101e-01,\n",
      "         1.7742e-01, -2.7626e-01, -3.1411e-01,  1.7631e-02,  3.2190e-01,\n",
      "         3.7291e-01,  2.4310e-01,  3.1665e-01,  1.8860e-01,  2.2580e-01,\n",
      "        -5.0789e-03,  1.4316e-01, -4.5206e-01,  6.0207e-01, -2.7828e-01,\n",
      "         5.4158e-02, -1.0913e-01, -2.7859e-01, -6.0596e-01, -1.3781e-01,\n",
      "        -1.9059e-01,  4.2602e-01, -2.8716e-01,  2.2630e-01, -1.6860e-01,\n",
      "        -9.9387e-02, -2.3894e-01, -7.6912e-02,  8.1173e-02,  5.0038e-01,\n",
      "         2.6606e-01,  1.6927e-01,  4.0572e-01,  1.4523e-01, -2.2695e-01,\n",
      "        -3.2934e-01, -3.3607e-01,  2.8036e-01, -1.2733e-01, -1.7859e-01,\n",
      "        -2.3151e-01,  3.6431e-01,  1.5069e+00,  1.7756e-02, -1.9430e-01,\n",
      "        -4.9444e-02,  2.2317e-01,  2.2725e-01, -1.0679e-01, -2.4852e-01,\n",
      "         3.1330e-01,  3.2021e-01,  1.4261e-01, -3.2262e-01,  2.3225e-01,\n",
      "        -3.1346e-02,  7.7263e-02, -4.0279e-01, -1.8989e-01,  3.6232e-01,\n",
      "         1.9014e-01, -3.3168e-01, -4.8909e-02,  2.4517e-01,  1.7927e-01,\n",
      "         3.0824e-01,  5.5156e-02,  1.6522e-01, -2.0542e-01,  2.9591e-01,\n",
      "        -2.3858e-01,  4.1037e-01,  4.6060e-01, -1.7871e-01,  2.0468e-01,\n",
      "         2.1003e-01,  6.9631e-02,  1.4103e-01,  1.7143e-01, -3.3942e-01,\n",
      "         3.0755e-01, -7.7534e-02, -5.1514e-03, -2.1013e-01, -2.1196e-01,\n",
      "         1.6663e-01, -2.3977e-01, -1.5511e-01,  3.7018e-01,  7.5590e-02,\n",
      "         2.5630e-01, -3.4357e-02, -1.4536e-01,  5.3429e-01,  6.0323e-01,\n",
      "         1.5891e-01,  2.7208e-01,  1.7320e-01, -1.7444e-01, -3.9451e-01,\n",
      "         4.1891e-01, -3.1795e-01, -4.8417e-02,  3.6538e-01,  7.9103e-02,\n",
      "         3.3758e-01,  2.4375e-01, -3.1531e-01, -1.2490e-01, -1.5365e-01,\n",
      "         4.3103e-01,  1.8449e-01,  4.5956e-01, -1.7718e-01,  4.5113e-01,\n",
      "         2.7752e-02, -3.2984e-01,  2.3545e-01, -2.3709e-01, -2.6831e-02,\n",
      "        -1.9654e-01, -2.0521e-01, -9.3321e-02, -9.2346e-02,  8.0321e-02,\n",
      "        -2.6918e-01, -8.8076e-02,  3.4239e-01, -4.0665e-03,  2.2624e-01,\n",
      "         1.7988e-01,  2.6217e-01,  2.9100e-01,  3.1839e-01,  3.3180e-01,\n",
      "         3.7771e-01,  1.1493e-01, -2.3912e-01, -1.0883e-01,  1.7101e-01,\n",
      "         3.3645e-01, -8.8343e-02, -5.9741e-02,  1.1604e-01, -5.4409e-02,\n",
      "        -2.9487e-01, -1.8762e-01,  3.5940e-01, -1.9403e-01,  2.9688e-01,\n",
      "        -2.7290e-01,  2.0787e-01, -1.5488e-01,  3.0128e-01,  1.0183e-01,\n",
      "         3.0283e-01,  2.6187e-01,  2.0423e-01, -2.7535e-01, -9.0575e-02,\n",
      "         2.0578e-01,  2.8182e-01, -2.2814e-01,  3.4879e-01, -1.1036e-01,\n",
      "         1.5063e-01, -1.2271e-01,  3.0094e-01,  6.6667e-01, -2.9499e-01,\n",
      "        -4.2971e-01,  2.5389e-01,  9.7503e-02,  4.4599e-01,  7.6113e-02,\n",
      "         1.4224e-01, -8.2404e-02,  4.4578e-01,  1.0711e-01,  1.1083e-02,\n",
      "         1.3274e-01,  4.1646e-01,  3.4401e-01,  2.7164e-01, -3.5840e-01,\n",
      "         1.0513e-01,  2.4209e-01, -2.5650e-02,  3.0007e-01, -1.3312e-01,\n",
      "         3.1898e-01,  4.2647e-01, -7.6398e-04, -1.6826e-01, -1.2465e-01,\n",
      "        -2.2957e-01,  2.8424e-01, -2.8746e-01,  1.4780e-01,  2.9163e-02,\n",
      "         2.7695e-01,  2.0721e-01, -2.3600e-01,  1.5833e-01,  5.0254e-01,\n",
      "        -2.2301e-01, -1.7899e-01,  4.7639e-01,  5.1641e-01,  4.8355e-01,\n",
      "         3.2271e-01, -2.1683e-01, -3.7347e-01,  6.9918e-01,  1.8678e-01,\n",
      "         6.6470e-01,  2.0930e-01, -1.1664e-01,  1.8928e-01, -2.9981e-01,\n",
      "         1.6058e-01,  3.2936e-01, -2.5366e-01, -1.9297e-01, -1.8531e-01,\n",
      "         3.8791e-01,  3.0624e-01,  4.8993e-01, -2.8346e-01, -2.0691e-01,\n",
      "        -1.4544e-01,  2.5651e-01, -1.8980e-01, -3.4219e-01, -1.0817e-01,\n",
      "         5.4294e-01, -3.3467e-02,  2.1371e-01,  4.2320e-01, -2.1863e-01,\n",
      "        -3.1441e-01, -1.6283e-01, -1.7398e-01,  2.9077e-01,  1.0299e-01,\n",
      "         2.9308e-01, -1.6903e-01,  2.2657e-01,  3.7302e-01,  5.5304e-01,\n",
      "         2.4431e-01, -1.7326e-01,  3.3798e-01,  2.5890e-01,  4.1547e-01,\n",
      "         1.7696e-01,  1.6554e-01,  1.3651e-01,  1.6195e-01,  5.0778e-01,\n",
      "        -2.5585e-01, -1.8214e-01,  3.2236e-01,  3.3633e-01, -1.8681e-01,\n",
      "         2.5198e-01, -1.4466e-01, -1.6255e-01,  1.4924e-01, -1.8050e-01,\n",
      "         4.4645e-01,  6.5917e-01,  1.9846e-01,  3.5441e-03,  4.3854e-02,\n",
      "         8.5625e-01,  2.3675e-01, -2.1392e-01, -2.2863e-01, -2.0787e-01,\n",
      "        -2.4713e-01,  3.4304e-01, -3.5820e-01, -2.0440e-01, -4.1410e-02,\n",
      "        -2.8124e-01, -1.7833e-01,  2.8527e-01,  8.3056e-02,  1.5993e-01,\n",
      "        -2.6334e-01, -1.6020e-02,  5.7103e-01,  2.9440e-01,  3.1788e-01,\n",
      "        -1.2114e-01,  2.4687e-01, -1.2078e-01, -1.1244e-01,  2.7837e-01,\n",
      "        -1.6701e-01,  3.7207e-02, -8.0388e-02, -1.2335e-01, -1.8499e-01,\n",
      "        -1.5037e-01,  1.4365e-01,  3.9081e-01,  4.8148e-01,  3.4222e-01,\n",
      "         7.3879e-02,  2.2796e-01,  6.0844e-02,  2.1728e-01,  3.6975e-01,\n",
      "         3.5159e-01, -4.1645e-01, -1.7427e-01, -3.4512e-01,  3.8800e-01,\n",
      "        -1.9050e-01,  1.4371e-01, -3.2587e-01, -1.3199e-01, -4.5050e-02,\n",
      "         2.2883e-01,  2.6382e-01, -2.2706e-01,  1.3469e-01, -1.0472e-01,\n",
      "        -3.3245e-01,  2.7325e-01, -3.7819e-01,  3.0851e-01,  5.2767e-01,\n",
      "        -8.9255e-02,  3.4062e-01,  4.4413e-04,  1.7450e-01,  4.6720e-01,\n",
      "         3.5978e-01, -3.9793e-01, -2.1512e-01, -2.3263e-01, -1.7321e-01,\n",
      "         3.4486e-01,  4.1299e-01,  3.0938e-02,  3.3500e-01, -3.1560e-03,\n",
      "        -2.9865e-01, -1.4625e+00, -9.8012e-02, -2.4043e-01,  3.2765e-01,\n",
      "        -1.9906e-01,  3.5112e-01,  4.1340e-01, -5.1585e-02,  3.0466e-01,\n",
      "        -9.2092e-02,  2.8678e-01,  4.4899e-01, -2.2106e-01,  4.9676e-01,\n",
      "         3.7348e-01,  2.9686e-01,  3.1068e-01,  3.0928e-02, -2.0017e-01,\n",
      "        -1.9282e-01, -2.8995e-01,  2.1477e-01, -1.5984e-01,  1.2666e-01,\n",
      "        -1.3201e-01,  3.2323e-01, -4.3752e-02,  2.9025e-01,  3.2418e-01,\n",
      "         6.2391e-02, -1.2523e-01,  4.0609e-01, -3.6170e-01,  2.4027e-01,\n",
      "        -3.6697e-01, -3.8663e-02,  3.6873e-01,  1.1361e-01,  3.4734e-01,\n",
      "         4.4872e-01, -1.2968e-01, -1.6122e-01, -8.3567e-02,  2.8099e-01,\n",
      "         1.5533e-01,  4.8083e-02,  2.8394e-01,  3.3250e-01, -1.2632e-01,\n",
      "         3.9128e-01,  2.9304e-01,  2.1636e-01,  2.7334e-01,  2.9482e-01,\n",
      "        -2.2127e-01, -4.8813e-01,  1.7477e-01,  3.6606e-01, -2.3342e-01,\n",
      "        -3.4502e-01, -1.7664e-01,  6.1239e-01, -3.0840e-01, -2.4923e-01,\n",
      "         5.1745e-01, -1.6211e-01, -2.8886e-01,  2.6668e-01, -4.0084e-02,\n",
      "         4.4098e-01, -1.3044e-01, -1.4970e-01,  5.6248e-01,  1.7226e-01,\n",
      "         3.8984e-01,  2.1294e-02,  2.2650e-01,  1.7169e-01, -1.3396e-02,\n",
      "         1.3517e-01, -1.3273e-01, -9.7026e-01, -3.6299e-01,  2.0350e-01,\n",
      "        -3.2499e-01,  1.1446e-01,  5.8854e-02, -1.1393e-01,  1.6254e-01,\n",
      "        -1.6630e-02, -4.9815e-02,  1.1064e-01, -1.0999e-01,  1.3746e-01,\n",
      "        -4.0445e-01,  2.0354e-01, -2.0029e-01,  2.7828e-01,  7.0115e-02,\n",
      "        -7.4485e-02, -3.1711e-01, -1.5554e-01, -3.3108e-01,  1.6979e-01,\n",
      "         3.3838e-01,  2.8970e-01, -1.3806e-01,  4.7803e-01,  2.8064e-01,\n",
      "        -4.5429e-02,  3.8397e-01,  2.9411e-01,  8.2865e-03,  4.9706e-01,\n",
      "        -8.9982e-02, -1.8782e-01,  1.4878e-01, -4.6184e-02,  2.7603e-01,\n",
      "         5.5914e-02, -2.9413e-01, -1.6638e-01,  1.3783e-01, -1.0809e-02,\n",
      "        -2.0474e-01,  3.5957e-01, -3.1878e-01, -2.6454e-01,  2.8889e-01,\n",
      "        -3.2722e-01, -1.1967e-01,  3.7089e-01,  1.3817e-01,  2.4711e-01,\n",
      "         1.3410e-01,  4.3482e-01, -1.2043e-01, -9.1686e-02, -2.9327e-01,\n",
      "         2.2992e-01,  3.0474e-01,  2.8517e-01,  1.7693e-01,  4.0895e-01,\n",
      "         1.8468e-01, -2.8496e-01,  3.0090e-01,  3.2440e-01,  4.8546e-01,\n",
      "         2.5820e-01,  3.9162e-01,  4.4743e-01, -3.0039e-01,  1.6349e-01,\n",
      "        -4.3414e-01,  2.3696e-01, -2.5321e-01, -2.5352e-01, -5.7491e-02,\n",
      "         2.5758e-01, -3.4549e-01, -1.4314e-01,  1.9363e-01, -2.1689e-01,\n",
      "         1.4741e-01,  3.5460e-01,  7.6415e-03, -1.3723e-01, -3.6182e-01,\n",
      "        -3.4953e-01,  3.3921e-02,  3.3331e-01,  2.7363e-01, -4.7957e-01,\n",
      "         7.6437e-02,  1.7814e-01,  9.7068e-02,  3.9068e-01,  2.0435e-01,\n",
      "        -2.0854e-01,  1.6268e-01, -1.0596e-01, -1.8507e-01,  2.7797e-01,\n",
      "         1.4957e-01,  3.2809e-02,  8.8581e-01,  1.7927e-01,  3.3608e-01,\n",
      "        -2.0538e-01,  1.8952e-01,  3.3213e-01,  1.2175e-02,  1.7972e-01,\n",
      "         3.1962e-01,  1.9531e-01, -3.8599e-01,  1.2770e-01,  8.9703e-02,\n",
      "         3.2829e-01, -4.0278e-02,  1.2672e-01,  1.9618e-01,  2.6590e-01,\n",
      "         3.8646e-01, -2.6673e-01, -1.7191e-01,  3.4580e-01,  2.3503e-01,\n",
      "         2.7523e-01,  4.5732e-02,  2.2173e-01,  2.2686e-02,  3.0655e-01,\n",
      "         2.3322e-02,  3.0158e-01, -3.4946e-02, -1.5740e-01,  2.0877e-01,\n",
      "        -1.2443e-01,  3.2736e-01,  4.3598e-01, -2.6642e-01,  2.3690e-01,\n",
      "        -1.6390e-01,  5.5801e-01,  1.5931e-01,  2.4012e-01,  2.6217e-01,\n",
      "         4.1573e-01,  3.0506e-01, -1.7347e-01, -8.4155e-02,  3.1101e-01,\n",
      "         4.4681e-01, -1.6268e-01, -1.8150e-01,  3.3103e-01,  1.9935e-01,\n",
      "         4.0458e-01,  2.8688e-01,  9.7072e-01, -9.0604e-02, -1.2935e-01,\n",
      "         3.0258e-01,  5.4714e-01,  2.6101e-01,  3.7248e-01,  3.2539e-01,\n",
      "         1.7793e-01,  2.1520e-01, -1.6420e-01,  2.6468e-01, -2.0426e-01,\n",
      "         4.5859e-01, -3.0426e-01,  2.4754e-01, -2.7398e-01, -6.1967e-02,\n",
      "         2.6496e-01, -1.0107e-01, -1.8115e-01,  3.7365e-01,  2.1818e-01,\n",
      "         3.3648e-01,  1.6618e-01,  2.6685e-01, -1.5546e-01,  2.1745e-01,\n",
      "        -1.5236e-01, -3.6364e-02, -2.4578e-01, -1.6300e-01,  5.8325e-02,\n",
      "        -1.1173e-01,  3.3063e-01, -1.7482e-01,  3.2570e-01, -5.8007e-02,\n",
      "        -7.9174e-03, -2.6682e-01,  2.0093e-01,  5.1152e-02,  2.3514e-01,\n",
      "         1.0374e-01,  1.7963e-01,  1.6511e-01,  2.0009e-01,  2.7566e-01,\n",
      "        -1.5090e-01,  2.5384e-01, -4.4236e-02], requires_grad=True)\n",
      "torch.Size([768])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0051, -0.0031, -0.0147,  ...,  0.0068, -0.0104,  0.0309],\n",
      "        [ 0.0147,  0.0323,  0.0277,  ...,  0.0160,  0.0347, -0.0269],\n",
      "        [-0.0261,  0.0071,  0.0003,  ...,  0.0258,  0.0306, -0.0349],\n",
      "        ...,\n",
      "        [ 0.0022, -0.0034,  0.0308,  ..., -0.0100, -0.0143,  0.0331],\n",
      "        [-0.0037,  0.0234,  0.0254,  ...,  0.0022, -0.0002, -0.0150],\n",
      "        [-0.0238,  0.0266,  0.0344,  ..., -0.0018, -0.0056, -0.0060]],\n",
      "       requires_grad=True)\n",
      "torch.Size([7, 768])\n"
     ]
    }
   ],
   "source": [
    "for p in prompt_model.verbalizer.parameters():\n",
    "    if p.requires_grad:\n",
    "        print(p)\n",
    "        print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "aa6f6c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "myverbalizer = SoftVerbalizer(tokenizer, plm, num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "80e5a883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5376"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in myverbalizer.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2c849ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5376"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "400741dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "589824"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "768*768"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3-7-NLP",
   "language": "python",
   "name": "3-7-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
