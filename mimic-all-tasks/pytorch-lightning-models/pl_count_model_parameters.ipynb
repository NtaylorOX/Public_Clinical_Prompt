{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9214025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.lib.histograms import _histogram_dispatcher\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "\n",
    "from torchnlp.encoders import Encoder\n",
    "from torchnlp.encoders.text import stack_and_pad_tensors\n",
    "from torchnlp.encoders.text.text_encoder import TextEncoder\n",
    "# from tokenizer import Tokenizer\n",
    "\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from bert_classifier import MimicBertModel, MimicDataset, MimicDataModule\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "from data_utils import FewShotSampler, Mimic_ICD9_Processor, Mimic_ICD9_Triage_Processor, Mimic_Mortality_Processor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1079ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir= \"./ckpts/icd9_triage/fewshot_32/emilyalsentzer/Bio_ClinicalBERT/version_28-03-2022--10-17-27/best-checkpoint.ckpt\"\n",
    "# hparams_file = \"/home/niallt/NLP_DPhil/NLP_Mimic_only/clinical-longformer/experiments/emilyalsentzer/Bio_ClinicalBERT/version_20-09-2021--11-08-38/hparams.yaml\"\n",
    "\n",
    "\n",
    "model = MimicBertModel.load_from_checkpoint(model_dir)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# # below works - but has to some funky shit\n",
    "\n",
    "# hparams = bios.read(hparams_file)\n",
    "\n",
    "# hparams = argparse.Namespace(**hparams)\n",
    "\n",
    "# model = Classifier(hparams = hparams)\n",
    "\n",
    "# checkpoint = torch.load(model_dir)\n",
    "\n",
    "# model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abdf1786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0141,  0.2775,  0.1987,  0.2107],\n",
       "        [-0.6086,  1.4512, -1.8213, -0.0802],\n",
       "        [ 0.7498, -0.4312, -1.1538, -0.1927]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(3, 4)\n",
    "x.shape\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e522b63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4221, 0.2021, 0.1868, 0.1890],\n",
       "        [0.0923, 0.7238, 0.0274, 0.1565],\n",
       "        [0.5418, 0.1663, 0.0808, 0.2111]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.softmax(x, dim=1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b126f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4221, 0.2021, 0.1868, 0.1890],\n",
       "        [0.0923, 0.7238, 0.0274, 0.1565],\n",
       "        [0.5418, 0.1663, 0.0808, 0.2111]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.softmax(x, dim=-1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04c1b818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5089, 0.2116, 0.7188, 0.4140],\n",
       "        [0.1004, 0.6842, 0.0953, 0.3095],\n",
       "        [0.3907, 0.1042, 0.1859, 0.2766]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.softmax(x, dim=0)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80e3061c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"bert_hidden_dim\":          768\n",
       "\"bert_model\":               emilyalsentzer/Bio_ClinicalBERT\n",
       "\"ce_class_weights\":         tensor([0.8571, 0.8571, 0.8571, 0.8571, 0.8571, 0.8571, 0.8571])\n",
       "\"class_labels\":             ['AcuteMedicine', 'Cardiology', 'Gastroenterology', 'Neurology', 'Obstetrics', 'Oncology', 'Respiratory']\n",
       "\"classifier_hidden_dim\":    768\n",
       "\"classifier_learning_rate\": 1e-05\n",
       "\"dropout\":                  0.1\n",
       "\"encoder_learning_rate\":    1e-05\n",
       "\"n_training_steps\":         840\n",
       "\"n_warmup_steps\":           100\n",
       "\"nr_frozen_epochs\":         0\n",
       "\"num_labels\":               7\n",
       "\"optimizer\":                adamw\n",
       "\"pretrained_dir\":           None\n",
       "\"reinit_n_layers\":          0\n",
       "\"weight_classes\":           False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8eb95b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0018, -0.0011,  0.0055,  ...,  0.0210,  0.0311, -0.0256],\n",
      "        [ 0.0187,  0.0003, -0.0002,  ..., -0.0237, -0.0259, -0.0028],\n",
      "        [-0.0312,  0.0288,  0.0182,  ..., -0.0226, -0.0232,  0.0299],\n",
      "        ...,\n",
      "        [ 0.0330,  0.0225,  0.0284,  ..., -0.0302, -0.0057, -0.0011],\n",
      "        [ 0.0235,  0.0031, -0.0190,  ...,  0.0357, -0.0026, -0.0057],\n",
      "        [ 0.0246, -0.0164,  0.0213,  ..., -0.0117,  0.0076,  0.0078]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-3.3294e-02,  1.4990e-02, -2.9678e-02,  1.9077e-02,  2.9570e-02,\n",
      "        -3.0759e-02, -2.9804e-02,  2.9669e-02,  3.2466e-02, -2.0307e-02,\n",
      "        -3.0033e-02, -2.9377e-02,  1.0488e-02,  1.9232e-02, -3.2398e-02,\n",
      "         9.1213e-04, -3.5529e-02,  2.3165e-02,  1.0563e-02,  1.7728e-02,\n",
      "         6.8345e-03,  2.8228e-03,  9.8166e-03, -6.7819e-03, -3.5817e-02,\n",
      "        -7.7066e-03,  3.0722e-02,  2.4454e-04, -3.0168e-02, -7.2411e-03,\n",
      "        -3.3801e-02,  2.3773e-02,  2.9274e-02, -1.8287e-02, -3.3189e-02,\n",
      "         1.2957e-03, -2.3545e-02, -2.7173e-02,  1.4208e-02, -1.7342e-02,\n",
      "         3.1453e-02, -2.3822e-02,  3.5243e-02, -8.4873e-03, -2.4666e-02,\n",
      "         3.2482e-02, -2.9773e-02, -3.0065e-02,  3.0381e-02,  3.4640e-02,\n",
      "         2.5836e-02,  2.0466e-02,  1.0781e-02,  5.9208e-03,  7.7086e-03,\n",
      "         1.6709e-02,  5.0567e-03,  4.5260e-03, -2.9543e-02,  1.9113e-02,\n",
      "        -3.5560e-02,  2.0912e-02,  1.0260e-02,  1.7608e-02, -8.2616e-03,\n",
      "         3.6054e-03, -3.5014e-02, -2.8932e-02,  1.9724e-02,  3.4208e-02,\n",
      "        -3.3775e-02, -9.4127e-03, -3.5142e-02, -4.7074e-03, -3.3359e-02,\n",
      "         9.6106e-04, -1.0161e-02,  6.2147e-04, -1.4319e-02,  3.7776e-03,\n",
      "        -2.9960e-02,  3.6137e-02, -2.1602e-02,  1.4931e-02,  5.2069e-03,\n",
      "         4.0048e-03,  2.0481e-02,  2.9465e-02,  1.5560e-03, -1.1143e-02,\n",
      "        -1.5696e-02,  3.6065e-02,  3.3662e-03,  1.5371e-02,  2.9030e-02,\n",
      "         3.2126e-02, -1.8945e-02, -2.8956e-03,  2.0401e-02, -2.5914e-02,\n",
      "         1.4488e-02,  3.1510e-02, -3.5008e-02, -1.9193e-02,  3.3326e-02,\n",
      "         5.2349e-03, -9.9488e-03,  6.4144e-03, -1.7717e-02, -5.5284e-04,\n",
      "        -2.3213e-02,  5.2012e-03,  5.9873e-03,  2.1997e-02, -2.1113e-02,\n",
      "        -5.4367e-03,  1.9377e-02, -1.6711e-02,  1.6253e-02,  2.4872e-02,\n",
      "        -2.0788e-02,  2.4330e-02, -3.1304e-02,  3.1836e-02, -1.4577e-03,\n",
      "         1.9293e-03,  1.2451e-02,  8.0306e-03,  9.8597e-03,  1.0585e-02,\n",
      "         3.5620e-02, -1.0928e-02,  1.1078e-02,  7.8706e-03,  6.9431e-03,\n",
      "        -2.8804e-03, -2.7227e-02,  2.4360e-02,  6.0094e-03,  1.5239e-02,\n",
      "        -2.7374e-02,  2.5871e-02,  2.1601e-02,  8.6205e-03,  1.1672e-02,\n",
      "         1.8116e-02,  1.7050e-02,  1.9553e-02,  1.2867e-02,  4.6659e-03,\n",
      "         2.3747e-02, -7.1647e-03,  2.6229e-02,  1.9120e-02, -1.3603e-02,\n",
      "         1.8019e-02, -9.1155e-04,  3.1442e-02, -4.0590e-03, -8.8493e-03,\n",
      "        -3.8681e-03,  1.4160e-02,  8.8031e-04, -6.4933e-03, -1.8557e-02,\n",
      "        -3.0188e-03,  1.1818e-02,  1.0169e-02, -2.0666e-04, -2.4444e-02,\n",
      "         3.0392e-02, -1.1252e-02,  1.2800e-02,  2.7110e-02, -3.3164e-02,\n",
      "         1.6461e-03,  1.0716e-02, -2.9250e-02,  2.4219e-02, -1.6220e-02,\n",
      "         6.8817e-03, -1.0915e-02, -2.1685e-06, -1.2810e-02, -2.2723e-03,\n",
      "         3.5673e-02, -1.6776e-03,  3.4990e-02,  3.2487e-02, -1.5117e-02,\n",
      "         4.1522e-03, -2.1714e-02, -2.8070e-02, -2.8850e-02, -1.3560e-02,\n",
      "         1.6130e-02,  1.0504e-02,  6.4989e-03, -2.7701e-02,  2.7466e-02,\n",
      "         4.3754e-03,  2.4950e-02, -2.4992e-03, -3.4487e-02,  1.1562e-02,\n",
      "         5.3088e-03, -1.2328e-02,  3.1320e-03,  2.1216e-02, -1.0210e-02,\n",
      "         2.2317e-02, -1.8792e-02, -3.5668e-02, -1.4854e-02, -1.3757e-03,\n",
      "         6.8446e-03,  2.2398e-02,  2.9217e-02,  1.7563e-02, -3.4905e-02,\n",
      "        -1.8573e-02, -1.3355e-02, -1.6164e-02, -2.3876e-02,  1.0039e-02,\n",
      "         1.7040e-02, -1.4515e-03,  9.6781e-03,  1.7146e-02,  1.3255e-02,\n",
      "         3.0566e-03, -3.2835e-02,  2.7419e-02, -1.9528e-02,  1.4313e-02,\n",
      "        -2.9036e-02,  2.7886e-02,  3.4981e-02,  1.2487e-02,  1.4558e-02,\n",
      "        -2.7155e-02,  1.1003e-03,  1.0767e-02,  2.4221e-02,  1.5151e-02,\n",
      "         3.2149e-02, -7.1597e-03,  1.2609e-02, -2.8099e-02, -3.7877e-03,\n",
      "         2.0193e-02,  2.3540e-03,  5.6835e-03,  2.5717e-02,  2.2651e-03,\n",
      "        -2.9151e-02, -3.2666e-02,  2.3315e-02,  2.5928e-03, -1.0592e-03,\n",
      "         1.9100e-02,  1.2024e-02, -7.9592e-03,  3.4301e-02,  1.8031e-02,\n",
      "        -1.7626e-02,  9.0295e-03,  1.1222e-02,  1.9257e-02,  3.3499e-02,\n",
      "         1.7233e-02,  3.0850e-02,  1.5294e-02,  2.3150e-02, -1.4111e-02,\n",
      "        -2.7209e-02, -1.5368e-02, -2.5443e-02, -2.1921e-02, -1.5550e-02,\n",
      "         2.1576e-02,  2.5070e-04, -9.2328e-03, -2.7107e-02,  1.0071e-02,\n",
      "        -2.3643e-02,  7.7236e-03, -7.1561e-03, -1.4553e-02,  2.9825e-03,\n",
      "         5.6561e-03,  2.8919e-02, -1.3820e-02,  1.9915e-02, -2.4370e-02,\n",
      "         1.5462e-02,  2.2913e-02,  3.5472e-02,  1.2107e-02, -2.7762e-02,\n",
      "         4.4686e-03,  2.9419e-02,  3.2730e-02,  9.9129e-03,  1.8450e-02,\n",
      "         2.5167e-02, -1.8669e-02,  7.8055e-03, -4.5437e-03, -1.9343e-02,\n",
      "         2.5056e-03, -1.9366e-02, -4.1361e-03,  4.2609e-03, -1.9167e-02,\n",
      "        -3.3018e-02,  2.4816e-02, -3.4496e-02,  1.9235e-02, -1.3914e-02,\n",
      "         9.9654e-03, -2.7541e-02, -2.7927e-02, -1.0497e-02,  2.0757e-02,\n",
      "        -2.2574e-02, -9.6079e-03,  2.3738e-03,  2.8895e-03,  3.2821e-02,\n",
      "         2.1770e-03, -3.5908e-02,  1.1178e-02, -3.5513e-02,  9.1718e-03,\n",
      "         5.1485e-03,  3.5541e-03,  3.4020e-02, -2.0771e-02,  4.2153e-03,\n",
      "        -1.8122e-02, -3.4794e-02,  1.3234e-02, -2.9032e-02,  6.8284e-03,\n",
      "         1.5854e-02,  1.2385e-02, -6.2594e-03, -3.9794e-04,  2.8716e-02,\n",
      "        -8.9666e-03,  6.6822e-03, -2.9231e-02,  1.0171e-02,  1.8723e-02,\n",
      "         1.6973e-02, -3.0858e-02, -2.0959e-02,  1.0348e-02,  2.5065e-02,\n",
      "         2.9022e-02,  2.8586e-02,  8.4444e-03, -3.2782e-02,  1.8661e-02,\n",
      "         1.0971e-03,  1.9748e-02, -2.4504e-02,  6.8512e-03,  8.2022e-03,\n",
      "        -9.5582e-04, -1.2827e-02,  3.4509e-02, -3.4067e-02, -1.9398e-02,\n",
      "        -3.0841e-02,  1.7264e-02,  2.2988e-02,  8.0333e-04, -8.9200e-04,\n",
      "        -2.0406e-02,  1.2251e-02, -1.4738e-02,  2.5063e-03, -1.5364e-02,\n",
      "        -1.2797e-02, -3.1344e-02,  1.8844e-02,  9.2944e-03, -2.2958e-02,\n",
      "         3.0948e-03,  3.5074e-02, -2.4154e-02, -2.2283e-02, -3.5493e-02,\n",
      "         3.1673e-02,  2.4676e-02,  1.2467e-02, -1.9365e-02,  8.1553e-03,\n",
      "        -3.3500e-02,  2.5413e-03,  2.6717e-02, -3.2119e-02,  4.2686e-03,\n",
      "        -9.4437e-03, -3.5793e-02,  3.2963e-02,  2.8925e-02,  6.5831e-03,\n",
      "        -9.2404e-03,  3.0848e-04, -1.0421e-02, -1.8556e-02, -9.3971e-03,\n",
      "         8.0675e-03, -2.7839e-02,  2.7808e-02, -3.5155e-03, -4.4186e-03,\n",
      "        -2.1705e-02, -3.5457e-02,  4.1694e-03, -5.2554e-03, -1.2423e-03,\n",
      "        -3.5982e-03, -2.7685e-02,  7.6270e-03, -1.2759e-02, -2.0260e-03,\n",
      "         8.6639e-03, -2.0477e-02,  8.5926e-03, -1.4245e-02, -1.9205e-02,\n",
      "        -3.4196e-02,  1.9137e-02, -2.3899e-02,  1.6679e-02,  9.6153e-03,\n",
      "        -3.0942e-02,  1.1647e-02, -3.5962e-02, -1.2438e-02,  2.9902e-02,\n",
      "        -2.2249e-02,  2.1174e-02,  1.1527e-02, -2.4042e-02, -1.4932e-02,\n",
      "        -2.3480e-02, -2.5409e-02,  1.3138e-02, -1.0696e-02, -5.6465e-03,\n",
      "        -2.0966e-02,  3.2238e-02,  1.6567e-02, -2.4448e-02, -3.1648e-02,\n",
      "        -8.1977e-03, -6.3494e-03,  1.5747e-02,  2.0734e-02, -2.6057e-03,\n",
      "        -1.1704e-02,  1.2249e-02,  3.6070e-02, -1.9452e-02, -6.8784e-03,\n",
      "        -2.6670e-02, -2.5380e-02, -2.4499e-02, -2.7633e-02, -3.5823e-02,\n",
      "        -2.5315e-02, -6.7327e-03, -2.7467e-02, -2.2788e-02,  2.4350e-02,\n",
      "        -3.4042e-02, -1.5946e-02,  1.3086e-02,  1.2722e-03, -7.8651e-03,\n",
      "         2.4137e-02, -2.6392e-02,  1.1108e-02,  3.4842e-02, -2.8126e-02,\n",
      "        -3.2437e-02, -6.4081e-03,  2.0303e-02, -1.6591e-02,  1.0205e-02,\n",
      "        -2.0894e-02, -1.6891e-02, -8.4929e-03, -2.7124e-02, -1.4487e-02,\n",
      "         2.3959e-02, -8.9488e-03,  1.9921e-02,  7.3044e-03,  1.3968e-02,\n",
      "        -3.5464e-02,  7.6825e-04, -3.3264e-02, -2.1561e-02, -2.5051e-02,\n",
      "        -2.5189e-02, -3.5388e-02, -6.3879e-03,  1.5834e-02, -1.3351e-03,\n",
      "        -2.4289e-04,  1.8421e-02, -2.8223e-02, -3.3328e-02, -2.5883e-02,\n",
      "         8.5049e-03, -3.2232e-02,  1.3529e-02, -3.2231e-02,  3.2004e-02,\n",
      "         2.7725e-02,  4.0772e-03,  2.7027e-02, -5.9576e-03, -1.8615e-02,\n",
      "         2.5421e-02, -7.8854e-03, -1.7927e-02,  1.1095e-02, -1.9141e-02,\n",
      "        -1.9212e-02,  2.1840e-02,  3.2494e-02, -1.2994e-02,  2.6717e-03,\n",
      "         2.8323e-02,  1.2640e-02, -3.5546e-02, -3.5633e-02, -2.1897e-02,\n",
      "         1.7586e-02,  2.2307e-02,  5.1752e-03, -2.3321e-02, -5.4728e-03,\n",
      "        -9.0818e-03,  2.5619e-02, -2.8691e-02,  1.5024e-02,  5.6047e-03,\n",
      "         1.1066e-02,  3.3579e-03,  9.3550e-03,  8.6920e-03, -2.4225e-02,\n",
      "        -2.1071e-02,  3.1761e-02,  1.2553e-02,  1.2169e-02,  1.5454e-02,\n",
      "         3.0133e-02,  3.9032e-03, -2.5341e-02,  8.2186e-03, -1.5020e-02,\n",
      "         2.0907e-02,  6.2155e-04, -5.8687e-03,  2.0056e-02, -1.7907e-02,\n",
      "        -1.3116e-02,  1.1003e-02,  2.0018e-03,  1.5867e-03, -1.4745e-02,\n",
      "         2.3726e-02, -1.0976e-03,  7.6199e-03,  2.0769e-02,  3.3206e-02,\n",
      "         1.3668e-02, -3.0968e-02,  2.4922e-02,  3.4404e-02, -8.2480e-03,\n",
      "        -2.1027e-02, -3.2041e-02,  2.0496e-03, -3.6330e-02, -2.0752e-02,\n",
      "         7.1353e-03,  7.7692e-03,  2.6883e-02,  2.5532e-02,  3.5855e-02,\n",
      "         2.8869e-03,  3.3101e-02, -2.5149e-02, -3.6143e-03,  2.8984e-03,\n",
      "         8.0865e-03,  1.6181e-02, -2.0248e-02,  9.4829e-03,  3.3695e-02,\n",
      "         2.4953e-03,  2.5124e-02,  3.0983e-02,  1.6743e-02,  3.1690e-02,\n",
      "        -2.4719e-03,  8.5762e-03,  6.2304e-03,  2.5409e-02, -1.1020e-02,\n",
      "         3.2620e-02,  4.4031e-03, -2.6505e-02,  1.7700e-02, -5.6128e-03,\n",
      "        -4.6486e-04, -3.5846e-02, -3.2503e-03,  1.1832e-02,  3.0883e-02,\n",
      "         1.3730e-02,  2.1793e-02, -3.4873e-02,  2.5879e-02, -2.8359e-03,\n",
      "        -2.1332e-02,  2.9397e-02,  3.4954e-02, -2.9612e-03, -1.1772e-02,\n",
      "        -3.1978e-02, -9.4543e-03, -3.2303e-02,  2.5271e-02,  7.7281e-03,\n",
      "        -6.0430e-03, -1.1925e-02, -2.9351e-02, -2.7553e-02, -3.1334e-02,\n",
      "        -1.2469e-02, -3.3923e-02, -1.0893e-02,  2.9337e-02,  8.2363e-03,\n",
      "        -9.1297e-03,  2.3865e-02,  2.0374e-02,  2.2468e-03, -3.0288e-02,\n",
      "         3.3809e-03, -2.6960e-02,  3.0693e-02,  1.0255e-02, -3.3149e-02,\n",
      "         1.2361e-02,  2.3143e-02, -3.5787e-02,  1.5178e-02,  2.0479e-03,\n",
      "         2.3341e-02,  1.2890e-02, -3.1673e-02,  3.3658e-03,  2.5913e-03,\n",
      "         1.9654e-02,  1.1117e-03, -4.9573e-03,  2.2163e-03,  7.3966e-03,\n",
      "         1.6247e-02, -2.6284e-03, -1.1967e-02,  5.2616e-03, -2.3101e-02,\n",
      "         3.4514e-02,  2.9410e-02, -1.3441e-02,  2.4173e-02,  1.9108e-02,\n",
      "        -9.1484e-03,  2.4474e-02,  1.3491e-02, -3.2313e-02,  2.3369e-02,\n",
      "        -1.4745e-02,  1.0649e-02,  5.5284e-03, -2.3207e-02, -6.8257e-03,\n",
      "        -1.5334e-03, -1.6664e-02, -2.0676e-02,  2.9168e-02, -2.5615e-03,\n",
      "         2.1828e-02,  3.0691e-02, -1.4017e-02,  1.3463e-02, -6.6523e-03,\n",
      "        -2.4065e-02, -1.7502e-02, -1.9467e-02,  9.7861e-04,  1.7213e-02,\n",
      "         1.3845e-02, -7.7940e-03,  2.0642e-02, -1.0338e-02, -1.9124e-02,\n",
      "         1.0481e-02, -1.6979e-02, -2.0651e-02,  2.3347e-02, -1.3676e-02,\n",
      "        -2.4180e-02,  1.3059e-02, -5.6493e-03, -2.2058e-03,  1.6247e-02,\n",
      "        -5.3473e-03, -2.8362e-02,  3.4763e-02,  4.1305e-03, -2.8344e-02,\n",
      "        -2.0607e-02, -9.9247e-04,  2.3893e-02,  2.9759e-02,  8.8861e-03,\n",
      "         7.5702e-03, -2.2282e-02,  1.4984e-02,  7.0310e-03, -1.6112e-02,\n",
      "         1.4978e-02, -3.4588e-02, -2.4658e-02,  3.4393e-02,  3.5304e-02,\n",
      "         3.2607e-02, -2.6211e-02, -3.1540e-02,  1.1142e-02, -3.4475e-02,\n",
      "         1.4064e-02,  2.5150e-02,  2.1033e-02, -3.0239e-02, -9.2711e-03,\n",
      "        -3.4262e-02, -1.4248e-02,  3.3360e-02, -2.9586e-02, -2.4455e-02,\n",
      "        -7.8858e-03, -2.9888e-02,  3.4574e-02], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0125,  0.0100, -0.0261,  ...,  0.0197, -0.0320, -0.0327],\n",
      "        [ 0.0279,  0.0223, -0.0276,  ...,  0.0357,  0.0095, -0.0129],\n",
      "        [ 0.0119, -0.0184, -0.0140,  ..., -0.0308,  0.0152,  0.0164],\n",
      "        ...,\n",
      "        [-0.0203, -0.0106,  0.0171,  ...,  0.0019,  0.0201,  0.0127],\n",
      "        [-0.0350,  0.0026, -0.0258,  ..., -0.0105, -0.0327, -0.0067],\n",
      "        [-0.0353, -0.0042, -0.0331,  ..., -0.0020,  0.0312,  0.0173]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0301, -0.0140,  0.0272, -0.0328,  0.0328,  0.0150, -0.0007],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.classification_head.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fa0191b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108906247"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6200da1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MimicBertModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classification_head): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=768, out_features=7, bias=True)\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d81cb816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for getting trainable parameters of a model\n",
    "\n",
    "def get_n_trainable_params(model_dir):\n",
    "    \n",
    "    \n",
    "    model = MimicBertModel.load_from_checkpoint(model_dir)\n",
    "    \n",
    "    # all trainable\n",
    "    num_total_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    # split into the plm and classisifcation head\n",
    "    num_plm_trainable = sum(p.numel() for p in model.bert.parameters() if p.requires_grad)\n",
    "    \n",
    "    num_classifier_trainable = sum(p.numel() for p in model.classification_head.parameters() if p.requires_grad)\n",
    "    \n",
    "    # assert sum of the two = total\n",
    "    assert num_plm_trainable+num_classifier_trainable== num_total_trainable\n",
    "    \n",
    "    print(f\"Number of trainable parameters of PLM: {num_plm_trainable}\\n\")\n",
    "    print('#'*50)\n",
    "    print(f\"Number of trainable parameters of classifier: {num_classifier_trainable}\\n\")\n",
    "    print('#'*50)\n",
    "    print(f\"Total number of trainable parameters of whole model: {num_total_trainable}\")\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5be940a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 10:22:31.631 | WARNING  | bert_classifier:__init__:226 - Building model based on following architecture. emilyalsentzer/Bio_ClinicalBERT\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2022-04-12 10:22:33.900 | INFO     | bert_classifier:__init__:248 - Classification head has hidden dimension: 2\n",
      "2022-04-12 10:22:33.901 | WARNING  | bert_classifier:__init__:263 - Freezing the PLM i.e. the encoder - will just be tuning the classification head!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters of PLM: 0\n",
      "\n",
      "##################################################\n",
      "Number of trainable parameters of classifier: 1559\n",
      "\n",
      "##################################################\n",
      "Total number of trainable parameters of whole model: 1559\n"
     ]
    }
   ],
   "source": [
    "# finetuned model\n",
    "model_dir= \"./ckpts/sensitivity/icd9_triage/fewshot_128_classhidden_2/frozen_plm/emilyalsentzer/Bio_ClinicalBERT/version_12-04-2022--09-46-40/best-checkpoint.ckpt\"\n",
    "\n",
    "get_n_trainable_params(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca8d4a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "777.4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3887/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439c5b31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3-7-NLP",
   "language": "python",
   "name": "3-7-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
