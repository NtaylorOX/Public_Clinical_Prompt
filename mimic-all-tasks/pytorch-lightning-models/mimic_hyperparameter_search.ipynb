{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f2d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade \"ray[tune]\"\n",
    "# !pip install \"lightning-bolts\"\n",
    "# !pip install \"torchvision\"\n",
    "\n",
    "# !pip install \"pytorch-lightning==1.4\" # need 1.4 for raytune - but 1.5.10 was used for all experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bb462c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pl_bolts.datamodules import MNISTDataModule\n",
    "import os\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from ray import tune\n",
    "\n",
    "import tempfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchnlp.encoders import LabelEncoder\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from loguru import logger\n",
    "\n",
    "from transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup, get_constant_schedule_with_warmup \n",
    "from transformers.optimization import Adafactor, AdafactorSchedule \n",
    "from transformers import RobertaTokenizerFast as RobertaTokenizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix, roc_auc_score \n",
    "\n",
    "\n",
    "from data_utils import FewShotSampler, Mimic_ICD9_Processor, Mimic_ICD9_Triage_Processor, Mimic_Mortality_Processor\n",
    "\n",
    "from bert_classifier import MimicBertModel, MimicDataset, MimicDataModule\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c194de1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine the mimic bert model to avoid certain logging etc\n",
    "\n",
    "class hyperMimicBertModel(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 bert_model,\n",
    "                 num_labels,\n",
    "                 class_labels = [],\n",
    "                 bert_hidden_dim=768,\n",
    "                 classifier_hidden_dim=768,\n",
    "                 n_training_steps=None,\n",
    "                 n_warmup_steps=5000,\n",
    "                 dropout = 0.1,\n",
    "                 nr_frozen_epochs = 0,\n",
    "                 config = None):\n",
    "\n",
    "        super().__init__()\n",
    "        logger.warning(f\"Building model based on following architecture. {bert_model}\")\n",
    "\n",
    "        # set all the relevant parameters\n",
    "        self.num_labels = num_labels\n",
    "        self.class_labels = class_labels\n",
    "\n",
    "        \n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps \n",
    "        self.nr_frozen_epochs = nr_frozen_epochs\n",
    "\n",
    "        \n",
    "        # get parameters from config\n",
    "        self.lr = config['lr']\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(f\"{bert_model}\", return_dict=True)\n",
    "        # nn.Identity does nothing if the dropout is set to None\n",
    "        self.classification_head = nn.Sequential(nn.Linear(bert_hidden_dim, classifier_hidden_dim),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(dropout) if dropout is not None else nn.Identity(),\n",
    "                                        nn.Linear(classifier_hidden_dim, num_labels))\n",
    "        \n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "        self._frozen = False\n",
    "        \n",
    "\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "\n",
    "\n",
    "\n",
    "    def unfreeze_encoder(self) -> None:\n",
    "        \"\"\" un-freezes the encoder layer. \"\"\"\n",
    "        if self._frozen:\n",
    "            \n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = True\n",
    "            self._frozen = False\n",
    "\n",
    "    def freeze_encoder(self) -> None:\n",
    "        \"\"\" freezes the encoder layer. \"\"\"\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        self._frozen = True\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        # obtaining the last layer hidden states of the Transformer\n",
    "        last_hidden_state = output.last_hidden_state  # shape: (batch_size, seq_length, bert_hidden_dim)\n",
    "\n",
    "        #         or can use the output pooler : output = self.classifier(output.pooler_output)\n",
    "        # As I said, the CLS token is in the beginning of the sequence. So, we grab its representation\n",
    "        # by indexing the tensor containing the hidden representations\n",
    "        CLS_token_state = last_hidden_state[:, 0, :]\n",
    "        # passing this representation through our custom head\n",
    "        logits = self.classification_head(CLS_token_state)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(logits, labels)\n",
    "        return loss, logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        print(\"inside training!\")\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train/loss\", loss, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"predictions\": outputs.detach(), \"labels\": labels.detach()}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"valid/loss\", loss, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"predictions\": outputs.detach(), \"labels\": labels.detach()}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"test/loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss \n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        logger.warning(\"on validation epoch end\")\n",
    "\n",
    "        # get class labels\n",
    "        class_labels = self.class_labels\n",
    "\n",
    "\n",
    "        labels = []\n",
    "        predictions = []\n",
    "        scores = []\n",
    "        for output in outputs:\n",
    "            \n",
    "            for out_labels in output[\"labels\"].to('cpu').detach().numpy():                                \n",
    "                labels.append(out_labels)\n",
    "            for out_predictions in output[\"predictions\"]:\n",
    "                \n",
    "                # the handling of roc_auc score differs for binary and multi class\n",
    "                if len(class_labels) > 2:\n",
    "                    scores.append(torch.nn.functional.softmax(out_predictions).cpu().tolist())\n",
    "                # append probas\n",
    "                else:\n",
    "                    scores.append(torch.nn.functional.softmax(out_predictions)[1].cpu().tolist())\n",
    "\n",
    "                # get predictied labels                               \n",
    "                predictions.append(np.argmax(out_predictions.to('cpu').detach().numpy(), axis = -1))\n",
    "\n",
    "            #use softmax to normalize, as the sum of probs should be 1\n",
    "        # get sklearn based metrics\n",
    "        acc = balanced_accuracy_score(labels, predictions)\n",
    "        f1_weighted = f1_score(labels, predictions, average = 'weighted')   \n",
    "\n",
    "        # log to tensorboard\n",
    "\n",
    "        self.logger.experiment.add_scalar('valid/balanced_accuracy',acc, self.current_epoch)\n",
    "\n",
    "        self.logger.experiment.add_scalar('valid/f1_weighted',f1_weighted, self.current_epoch)\n",
    "\n",
    "\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        # get class labels\n",
    "        class_labels = self.class_labels\n",
    "\n",
    "\n",
    "        labels = []\n",
    "        predictions = []\n",
    "        scores = []\n",
    "        for output in outputs:\n",
    "            \n",
    "            for out_labels in output[\"labels\"].to('cpu').detach().numpy():                                \n",
    "                labels.append(out_labels)\n",
    "            for out_predictions in output[\"predictions\"]:\n",
    "                \n",
    "                # the handling of roc_auc score differs for binary and multi class\n",
    "                if len(class_labels) > 2:\n",
    "                    scores.append(torch.nn.functional.softmax(out_predictions).cpu().tolist())\n",
    "                # append probas\n",
    "                else:\n",
    "                    scores.append(torch.nn.functional.softmax(out_predictions)[1].cpu().tolist())\n",
    "\n",
    "                # get predictied labels                               \n",
    "                predictions.append(np.argmax(out_predictions.to('cpu').detach().numpy(), axis = -1))\n",
    "\n",
    "            #use softmax to normalize, as the sum of probs should be 1\n",
    "        # get sklearn based metrics\n",
    "        acc = balanced_accuracy_score(labels, predictions)\n",
    "        f1_weighted = f1_score(labels, predictions, average = 'weighted')\n",
    "\n",
    "        # log to tensorboard\n",
    "\n",
    "        self.logger.experiment.add_scalar('test/balanced_accuracy',acc, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar('test/f1_weighted',f1_weighted, self.current_epoch)\n",
    "\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\" Pytorch lightning hook \"\"\"        \n",
    "        logger.warning(f\"On epoch {self.current_epoch}. Number of frozen epochs is: {self.nr_frozen_epochs}\")\n",
    "        if self.current_epoch + 1 >= self.nr_frozen_epochs:\n",
    "            logger.warning(\"unfreezing PLM(encoder)\")\n",
    "            self.unfreeze_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d38da2e",
   "metadata": {},
   "source": [
    "# TODO - Adapt below to work with our mimic models and tasks\n",
    "\n",
    "We will be running a hyperparameter search for just one of  the mimic tasks - lets go with icd9_50 as this has the \n",
    "highest variability in performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fdd98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mimic icd9_50 specific parameters\n",
    "\n",
    "dataset = \"icd9_50\"\n",
    "root_data_dir = \"../\"\n",
    "\n",
    "n_labels = 50\n",
    "warmup_steps = 50\n",
    "total_training_steps = 25000\n",
    "label_col = \"label\"\n",
    "max_tokens = 512\n",
    "pretrained_model_name = \"emilyalsentzer/Bio_ClinicalBERT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15ca49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a search space!\n",
    "config = {\n",
    " \"batch_size\": tune.choice([2, 4, 8]),\n",
    " \"grad_accum_steps\": tune.choice([2,10,30]),   \n",
    " \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd09190",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(f\"{pretrained_model_name}\")\n",
    "\n",
    "# TODO update the dataloading to use the custom dataprocessors from data_utils in this folder\n",
    "\n",
    "if dataset == \"icd9_50\":\n",
    "    logger.warning(f\"Using the following dataset: {dataset} \")\n",
    "    Processor = Mimic_ICD9_Processor\n",
    "    # update data_dir\n",
    "    data_dir = f\"{root_data_dir}/mimic3-icd9-data/intermediary-data/top_50_icd9/\"\n",
    "\n",
    "    # are we doing any downsampling or balancing etc\n",
    "    class_weights = False\n",
    "    balance_data = False\n",
    "\n",
    "    # get different splits - the processor will return a dataframe and class_labels for each, but we only need training class_labels\n",
    "    train_df, class_labels = Processor().get_examples(data_dir = data_dir, mode = \"train\", class_weights = class_weights, balance_data = balance_data)\n",
    "    val_df,_ = Processor().get_examples(data_dir = data_dir, mode = \"valid\", class_weights = class_weights, balance_data = balance_data)\n",
    "    test_df,_ = Processor().get_examples(data_dir = data_dir, mode = \"test\", class_weights = class_weights, balance_data = balance_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3c638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hyperMimicBertModel(bert_model=pretrained_model_name,\n",
    "                             num_labels=n_labels,\n",
    "                             n_warmup_steps=warmup_steps,\n",
    "                             n_training_steps=total_training_steps                           \n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1914c19f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f955bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4c4c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56cf656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_mimic(config, dataset = \"icd9_50\",\n",
    "                pretrained_model_name =\"emilyalsentzer/Bio_ClinicalBERT\" ,\n",
    "                root_data_dir = \"../\",max_tokens = 512, label_col = \"label\",\n",
    "                n_labels = 50, num_epochs=5, warmup_steps = 50, total_training_steps = 20000,\n",
    "                num_gpus=0, data_dir = \"~/ray_tune_results/\"):\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(f\"{pretrained_model_name}\")\n",
    "\n",
    "# TODO update the dataloading to use the custom dataprocessors from data_utils in this folder\n",
    "\n",
    "    if dataset == \"icd9_50\":\n",
    "        logger.warning(f\"Using the following dataset: {dataset} \")\n",
    "        Processor = Mimic_ICD9_Processor\n",
    "        # update data_dir\n",
    "        root_data_dir = f\"{root_data_dir}/mimic3-icd9-data/intermediary-data/top_50_icd9/\"\n",
    "\n",
    "        # are we doing any downsampling or balancing etc\n",
    "        class_weights = False\n",
    "        balance_data = False\n",
    "\n",
    "        # get different splits - the processor will return a dataframe and class_labels for each, but we only need training class_labels\n",
    "        train_df, class_labels = Processor().get_examples(data_dir = root_data_dir, mode = \"train\", class_weights = class_weights, balance_data = balance_data)\n",
    "        val_df,_ = Processor().get_examples(data_dir = root_data_dir, mode = \"valid\", class_weights = class_weights, balance_data = balance_data)\n",
    "        test_df,_ = Processor().get_examples(data_dir = root_data_dir, mode = \"test\", class_weights = class_weights, balance_data = balance_data)\n",
    "\n",
    "        # load model\n",
    "        model = hyperMimicBertModel(bert_model=pretrained_model_name,\n",
    "                                 num_labels=n_labels,\n",
    "                                 n_warmup_steps=warmup_steps,\n",
    "                                 n_training_steps=total_training_steps,\n",
    "                                    config = config\n",
    "\n",
    "                                 )\n",
    "    \n",
    "    \n",
    "\n",
    "    # push data through pipeline\n",
    "    # instantiate datamodule\n",
    "    data_module = MimicDataModule(\n",
    "        train_df,\n",
    "        val_df,\n",
    "        test_df,\n",
    "        tokenizer,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        max_token_len=max_tokens,\n",
    "        label_col = label_col,\n",
    "        num_workers=1,\n",
    "    )\n",
    "    metrics = {\"loss\": \"valid/loss\", \"acc\": \"valid/balanced_accuracy\"}\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=num_epochs,\n",
    "        gpus=num_gpus,\n",
    "        progress_bar_refresh_rate=0,\n",
    "        callbacks=[TuneReportCallback(metrics, on=\"validation_end\")])\n",
    "    \n",
    "    trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce6eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff0beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "num_epochs = 5\n",
    "gpus_per_trial = 0 # set this to higher if using GPU\n",
    "\n",
    "# Defining a search space!\n",
    "config = {\n",
    " \"batch_size\": 2,\n",
    " \"grad_accum_steps\": 2,   \n",
    " \"lr\": 1e-3,\n",
    "}\n",
    "\n",
    "train_mimic(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0795eda9",
   "metadata": {},
   "source": [
    "# bug below may suggest we want to edit the bert classifier for this hyperparameter search to not log so much?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62b743f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ray.cloudpickle.dumps(<class 'ray.tune.function_runner.wrap_function.<locals>.ImplicitFunc'>) failed.\nTo check which non-serializable variables are captured in scope, re-run the ray script with 'RAY_PICKLE_VERBOSE_DEBUG=1'. Other options: \n-Try reproducing the issue by calling `pickle.dumps(trainable)`. \n-If the error is typing-related, try removing the type annotations and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8277/1899552864.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tune_m\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     )\n",
      "\u001b[0;32m~/venvs/3-7-NLP/lib/python3.7/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, queue_trials, loggers, _remote)\u001b[0m\n\u001b[1;32m    454\u001b[0m                 \u001b[0mexport_formats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexport_formats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0mmax_failures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_failures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                 restore=restore)\n\u001b[0m\u001b[1;32m    457\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ignoring some parameters passed into tune.run.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/3-7-NLP/lib/python3.7/site-packages/ray/tune/experiment.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, run, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, sync_config, trial_name_creator, trial_dirname_creator, log_to_file, checkpoint_freq, checkpoint_at_end, keep_checkpoints_num, checkpoint_score_attr, export_formats, max_failures, restore)\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;34m\"checkpointable function. You can specify checkpoints \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \"within your trainable function.\")\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_identifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_identifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/3-7-NLP/lib/python3.7/site-packages/ray/tune/experiment.py\u001b[0m in \u001b[0;36mregister_if_needed\u001b[0;34m(cls, run_object)\u001b[0m\n\u001b[1;32m    263\u001b[0m                              \u001b[0;34m\"\\n-Try reproducing the issue by calling \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                              \u001b[0;34m\"`pickle.dumps(trainable)`. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m                              \u001b[0;34m\"\\n-If the error is typing-related, try removing \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m                              \"the type annotations and try again.\")\n\u001b[1;32m    267\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_msg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ray.cloudpickle.dumps(<class 'ray.tune.function_runner.wrap_function.<locals>.ImplicitFunc'>) failed.\nTo check which non-serializable variables are captured in scope, re-run the ray script with 'RAY_PICKLE_VERBOSE_DEBUG=1'. Other options: \n-Try reproducing the issue by calling `pickle.dumps(trainable)`. \n-If the error is typing-related, try removing the type annotations and try again."
     ]
    }
   ],
   "source": [
    "num_samples = 10\n",
    "num_epochs = 5\n",
    "gpus_per_trial = 0 # set this to higher if using GPU\n",
    "\n",
    "# Defining a search space!\n",
    "config = {\n",
    " \"batch_size\": tune.choice([2, 4, 8]),   \n",
    " \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "}\n",
    "\n",
    "trainable = tune.with_parameters(\n",
    "    train_mimic,   \n",
    "    num_epochs=num_epochs,\n",
    "    num_gpus=gpus_per_trial,\n",
    "    data_dir = \"~/ray_tune_results\")\n",
    "\n",
    "analysis = tune.run(\n",
    "    trainable,\n",
    "    resources_per_trial={\n",
    "        \"cpu\": 1,\n",
    "        \"gpu\": gpus_per_trial\n",
    "    },\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    config=config,\n",
    "    num_samples=num_samples,\n",
    "    name=\"tune_m\"\n",
    "    \n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "print(analysis.best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004974dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dumps(trainable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3-7-NLP",
   "language": "python",
   "name": "3-7-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
